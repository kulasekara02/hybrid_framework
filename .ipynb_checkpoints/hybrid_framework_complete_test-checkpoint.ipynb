{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting International Student Academic Success Using a Hybrid Machine Learning Model\n",
    "\n",
    "## Complete Framework: LSTM + Random Forest \n",
    "\n",
    "**Thesis Support**: This notebook implements a novel, explainable hybrid prediction framework combining temporal deep learning (LSTM) with ensemble methods (Random Forest) for international students in higher education.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Research Contributions & Uniqueness**\n",
    "\n",
    "### **1. Novel Hybrid Architecture**\n",
    "- **LSTM for Temporal Patterns**: Captures week-by-week engagement evolution (32 weeks)\n",
    "- **Random Forest for Static Features**: Handles demographic, cultural, and academic factors\n",
    "- **Late Fusion Strategy**: Combines temporal embeddings with static features for final prediction\n",
    "\n",
    "### **2. Cultural Adaptation Integration**\n",
    "- **Cultural Distance Metrics**: Quantifies home-host country differences\n",
    "- **Teaching Style Adaptation**: Measures pedagogical adjustment challenges\n",
    "- **Language Proficiency Impact**: Models language barriers on academic success\n",
    "\n",
    "### **3. Explainability & Actionability**\n",
    "- **Feature Importance Analysis**: Identifies key success/risk drivers\n",
    "- **Root Cause Detection**: Pinpoints specific barriers for at-risk students\n",
    "- **Intervention Recommendations**: Generates personalized support strategies\n",
    "\n",
    "### **4. Risk Stratification System**\n",
    "- **Multi-level Risk Assessment**: Low/Medium/High risk categorization\n",
    "- **Early Warning Indicators**: Detects at-risk students by week 8-12\n",
    "- **Cluster-based Profiling**: Groups students by behavioral patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Dataset Information**\n",
    "\n",
    "### **Current Dataset: Enhanced Synthetic/Simulated Data**\n",
    "\n",
    "**Purpose**: Framework validation and model development before real-world deployment\n",
    "\n",
    "**Characteristics**:\n",
    "- **Sample Size**: 1,783 international students\n",
    "- **Temporal Coverage**: 32 weeks of engagement tracking\n",
    "- **Institutions**: Multi-institutional (Latvia-focused with global comparison)\n",
    "- **Countries**: 15+ home countries (India, Nigeria, China, Bangladesh, Brazil, etc.)\n",
    "- **Features**: 40+ static + 4 temporal variables\n",
    "\n",
    "**Why Synthetic Data?**\n",
    "1. **Privacy Compliance**: Avoids GDPR/FERPA constraints during development\n",
    "2. **Controlled Testing**: Validates model under known conditions\n",
    "3. **Generalization Assessment**: Tests framework transferability\n",
    "4. **Ethical Research**: No student privacy risks during experimentation\n",
    "\n",
    "**Next Steps for Real Deployment**:\n",
    "- Partner with Latvian universities for anonymized real data\n",
    "- Conduct validation survey (planned: 200-300 students)\n",
    "- Compare with Open University Learning Analytics Dataset (OULAD)\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ **Methodology Overview**\n",
    "\n",
    "1. **Data Preprocessing**: Handle missing values, encode categories, normalize features\n",
    "2. **Feature Engineering**: Extract temporal patterns, create interaction features\n",
    "3. **LSTM Training**: Capture temporal engagement dynamics (32-week sequences)\n",
    "4. **Hybrid Integration**: Fuse LSTM embeddings with static features\n",
    "5. **Random Forest Classification**: Final risk prediction with explainability\n",
    "6. **Validation & Analysis**: Performance metrics, confusion matrices, ROC curves\n",
    "7. **Intervention Generation**: Root cause analysis and recommendation engine\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Master's Thesis Research  \n",
    "**Date**: November 2024 (Updated: November 2025)  \n",
    "**Framework Version**: 2.0 (Enhanced Explainability & Cultural Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Alternative: Using Publicly Available Real Datasets\n",
    "\n",
    "### **Option 1: Open University Learning Analytics Dataset (OULAD)** ‚≠ê RECOMMENDED\n",
    "\n",
    "**Source**: https://analyse.kmi.open.ac.uk/open_dataset\n",
    "\n",
    "**Description**: Real student data from Open University (UK)\n",
    "- **Size**: 32,593 students across 7 courses\n",
    "- **Temporal Data**: Daily VLE (Virtual Learning Environment) engagement\n",
    "- **Features**: Demographics, assessment scores, registration info, engagement metrics\n",
    "- **Outcome**: Pass/Fail/Withdrawn/Distinction\n",
    "- **Free**: Yes, publicly available after registration\n",
    "- **Format**: CSV files\n",
    "\n",
    "**Why OULAD?**\n",
    "- ‚úÖ Real student data (not synthetic)\n",
    "- ‚úÖ Large sample size (32K+ students)\n",
    "- ‚úÖ Temporal engagement tracking (similar to your framework)\n",
    "- ‚úÖ Peer-reviewed and widely used in research\n",
    "- ‚úÖ No privacy concerns (already anonymized)\n",
    "- ‚úÖ Can validate your hybrid model on real data\n",
    "\n",
    "**Files in OULAD**:\n",
    "1. `studentInfo.csv` - Demographics, registration details\n",
    "2. `studentVle.csv` - Daily VLE engagement (temporal data)\n",
    "3. `studentAssessment.csv` - Assessment scores\n",
    "4. `studentRegistration.csv` - Course registration info\n",
    "5. `courses.csv`, `assessments.csv`, `vle.csv` - Metadata\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: Student Performance Dataset (UCI)**\n",
    "\n",
    "**Source**: https://archive.ics.uci.edu/ml/datasets/Student+Performance\n",
    "\n",
    "**Description**: Portuguese secondary school students\n",
    "- **Size**: 649 students (Math) + 395 students (Portuguese)\n",
    "- **Features**: 33 variables (demographic, social, school-related)\n",
    "- **Outcome**: Final grades (G1, G2, G3)\n",
    "- **Free**: Yes, open access\n",
    "\n",
    "**Limitations**: \n",
    "- ‚ùå No temporal engagement data\n",
    "- ‚ùå Smaller sample\n",
    "- ‚ö†Ô∏è Secondary school (not higher education)\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 3: Coursera/EdX MOOC Datasets**\n",
    "\n",
    "**Source**: Various platforms release anonymized data\n",
    "- **Coursera**: Some courses release data via research partnerships\n",
    "- **EdX**: Harvard/MIT released several course datasets\n",
    "- **Size**: Varies (10K-100K+ students per course)\n",
    "\n",
    "**Example**: \n",
    "- HarvardX-MITx Person-Course dataset (290K students, 13M records)\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 4: Kaggle Education Datasets**\n",
    "\n",
    "**Source**: https://www.kaggle.com/datasets\n",
    "\n",
    "**Popular Datasets**:\n",
    "1. **Student Performance** - Multiple datasets available\n",
    "2. **Online Education** - Virtual classroom engagement\n",
    "3. **Dropout Prediction** - University student data\n",
    "\n",
    "**Pros**: \n",
    "- ‚úÖ Easy to access\n",
    "- ‚úÖ Community support\n",
    "- ‚úÖ Variety of formats\n",
    "\n",
    "**Cons**: \n",
    "- ‚ö†Ô∏è Variable quality\n",
    "- ‚ö†Ô∏è May lack temporal data\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Integrate Real Datasets into Your Code**\n",
    "\n",
    "Below, I'll show you how to adapt your framework to use OULAD (the best option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION TO USE REAL DATASET: Open University Learning Analytics Dataset (OULAD)\n",
    "# ============================================================================\n",
    "# \n",
    "# INSTRUCTIONS TO USE OULAD:\n",
    "# 1. Download from: https://analyse.kmi.open.ac.uk/open_dataset\n",
    "# 2. Extract files to: ./uploads/oulad/\n",
    "# 3. Uncomment and run this cell\n",
    "# \n",
    "# Alternatively, run this cell to download automatically (requires internet)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Set to True to use OULAD instead of synthetic data\n",
    "USE_OULAD = True  # ‚úÖ NOW USING REAL DATA!\n",
    "\n",
    "if USE_OULAD:\n",
    "    print('='*80)\n",
    "    print('üåê LOADING OPEN UNIVERSITY LEARNING ANALYTICS DATASET (OULAD)')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    oulad_dir = './uploads/oulad/'\n",
    "    os.makedirs(oulad_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if data already exists\n",
    "    required_files = [\n",
    "        'studentInfo.csv',\n",
    "        'studentVle.csv', \n",
    "        'studentAssessment.csv',\n",
    "        'studentRegistration.csv'\n",
    "    ]\n",
    "    \n",
    "    files_exist = all(os.path.exists(os.path.join(oulad_dir, f)) for f in required_files)\n",
    "    \n",
    "    if not files_exist:\n",
    "        print('\\nüì• OULAD files not found. Downloading...')\n",
    "        print('Note: Download may take 2-5 minutes depending on connection.')\n",
    "        \n",
    "        # OULAD download URL\n",
    "        oulad_url = 'https://analyse.kmi.open.ac.uk/open_dataset/download'\n",
    "        zip_path = os.path.join(oulad_dir, 'oulad.zip')\n",
    "        \n",
    "        try:\n",
    "            # Download the dataset\n",
    "            print('‚è≥ Downloading OULAD dataset...')\n",
    "            urllib.request.urlretrieve(oulad_url, zip_path)\n",
    "            \n",
    "            # Extract\n",
    "            print('üì¶ Extracting files...')\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(oulad_dir)\n",
    "            \n",
    "            # Clean up zip file\n",
    "            os.remove(zip_path)\n",
    "            print('‚úÖ Download complete!')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'‚ö†Ô∏è Automatic download failed: {e}')\n",
    "            print('\\nüìù Please download manually:')\n",
    "            print('   1. Visit: https://analyse.kmi.open.ac.uk/open_dataset')\n",
    "            print('   2. Register (free) and download the dataset')\n",
    "            print('   3. Extract to: ./uploads/oulad/')\n",
    "            print('   4. Re-run this cell')\n",
    "            USE_OULAD = False\n",
    "    \n",
    "    if USE_OULAD:\n",
    "        print('\\nüìÇ Loading OULAD files...')\n",
    "        \n",
    "        # Load OULAD data\n",
    "        oulad_students = pd.read_csv(os.path.join(oulad_dir, 'studentInfo.csv'))\n",
    "        oulad_vle = pd.read_csv(os.path.join(oulad_dir, 'studentVle.csv'))\n",
    "        oulad_assessment = pd.read_csv(os.path.join(oulad_dir, 'studentAssessment.csv'))\n",
    "        oulad_registration = pd.read_csv(os.path.join(oulad_dir, 'studentRegistration.csv'))\n",
    "        \n",
    "        print(f'\\n‚úÖ OULAD Data Loaded:')\n",
    "        print(f'   Students: {len(oulad_students):,}')\n",
    "        print(f'   VLE interactions: {len(oulad_vle):,}')\n",
    "        print(f'   Assessment records: {len(oulad_assessment):,}')\n",
    "        print(f'   Registrations: {len(oulad_registration):,}')\n",
    "        \n",
    "        print('\\nüìä Sample of OULAD Student Data:')\n",
    "        print(oulad_students.head())\n",
    "        \n",
    "        print('\\nüìä Available Features in OULAD:')\n",
    "        print('Student Info columns:', list(oulad_students.columns))\n",
    "        print('VLE columns:', list(oulad_vle.columns))\n",
    "        \n",
    "        print('\\nüí° Next Step: Run the OULAD preprocessing cell below to adapt data to your framework')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è Using synthetic dataset (current default)')\n",
    "    print('‚ÑπÔ∏è To use real OULAD data, set USE_OULAD = True above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OULAD DATA PREPROCESSING - Adapt to Your Framework Format\n",
    "# ============================================================================\n",
    "# This cell transforms OULAD data to match your framework's expected format\n",
    "# ============================================================================\n",
    "\n",
    "if USE_OULAD:\n",
    "    print('='*80)\n",
    "    print('üîÑ PREPROCESSING OULAD DATA FOR YOUR FRAMEWORK')\n",
    "    print('='*80)\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 1. PREPARE STATIC FEATURES (Demographics + Academic Summary)\n",
    "    # -------------------------------------------------------------------\n",
    "    print('\\n1Ô∏è‚É£ Creating static features...')\n",
    "    \n",
    "    # Map OULAD fields to your framework fields\n",
    "    df_static = oulad_students.copy()\n",
    "    \n",
    "    # Create student_id\n",
    "    df_static['student_id'] = df_static['id_student'].astype(str) + '_' + df_static['code_module'] + '_' + df_static['code_presentation']\n",
    "    \n",
    "    # Map demographic features\n",
    "    df_static['age_band'] = df_static['age_band']  # Keep as is or convert to numeric\n",
    "    df_static['gender'] = df_static['gender']\n",
    "    df_static['region'] = df_static['region']  # Similar to 'country_home'\n",
    "    df_static['highest_education'] = df_static['highest_education']\n",
    "    df_static['study_level'] = df_static['highest_education']  # Map to your study_level\n",
    "    \n",
    "    # Map to numeric age (midpoint of band)\n",
    "    age_mapping = {\n",
    "        '0-35': 27,\n",
    "        '35-55': 45,\n",
    "        '55<=': 60\n",
    "    }\n",
    "    df_static['age'] = df_static['age_band'].map(age_mapping).fillna(27)\n",
    "    \n",
    "    # Disability as support needs indicator\n",
    "    df_static['has_disability'] = df_static['disability'].map({'Y': 1, 'N': 0})\n",
    "    \n",
    "    # Number of previous attempts\n",
    "    df_static['num_of_prev_attempts'] = df_static['num_of_prev_attempts']\n",
    "    \n",
    "    # Studied credits as workload indicator\n",
    "    df_static['studied_credits'] = df_static['studied_credits']\n",
    "    \n",
    "    # IMD (Index of Multiple Deprivation) as socioeconomic indicator\n",
    "    imd_mapping = {\n",
    "        '0-10%': 5, '10-20%': 15, '20-30%': 25, '30-40%': 35,\n",
    "        '40-50%': 45, '50-60%': 55, '60-70%': 65, '70-80%': 75,\n",
    "        '80-90%': 85, '90-100%': 95\n",
    "    }\n",
    "    df_static['imd_band_numeric'] = df_static['imd_band'].map(imd_mapping).fillna(50)\n",
    "    \n",
    "    # Final result as target variable\n",
    "    df_static['success_label'] = df_static['final_result'].map({\n",
    "        'Pass': 1,\n",
    "        'Distinction': 1,\n",
    "        'Fail': 0,\n",
    "        'Withdrawn': 0\n",
    "    })\n",
    "    \n",
    "    # Course identifiers\n",
    "    df_static['institution'] = 'Open_University_UK'\n",
    "    df_static['program_id'] = df_static['code_module']\n",
    "    df_static['cohort_year'] = df_static['code_presentation'].str[:4]\n",
    "    \n",
    "    print(f'   ‚úì Static features created for {len(df_static):,} students')\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 2. PREPARE TEMPORAL FEATURES (Weekly VLE Engagement)\n",
    "    # -------------------------------------------------------------------\n",
    "    print('\\n2Ô∏è‚É£ Creating temporal features...')\n",
    "    \n",
    "    # Aggregate VLE data by student and week\n",
    "    df_temporal = oulad_vle.copy()\n",
    "    df_temporal['student_id'] = (df_temporal['id_student'].astype(str) + '_' + \n",
    "                                  df_temporal['code_module'] + '_' + \n",
    "                                  df_temporal['code_presentation'])\n",
    "    \n",
    "    # Convert date to week number (OULAD uses days, convert to weeks)\n",
    "    df_temporal['week_index'] = (df_temporal['date'] // 7).astype(int)\n",
    "    \n",
    "    # Aggregate clicks per student per week\n",
    "    temporal_aggregated = df_temporal.groupby(['student_id', 'week_index']).agg({\n",
    "        'sum_click': 'sum'  # Total clicks in that week\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename to match your framework\n",
    "    temporal_aggregated['weekly_engagement'] = temporal_aggregated['sum_click']\n",
    "    \n",
    "    # Add placeholder temporal features (you can enhance these)\n",
    "    temporal_aggregated['weekly_attendance'] = temporal_aggregated['weekly_engagement'] / temporal_aggregated['weekly_engagement'].max()\n",
    "    temporal_aggregated['weekly_assignments_submitted'] = 0  # OULAD doesn't have this directly\n",
    "    temporal_aggregated['weekly_quiz_attempts'] = 0  # OULAD doesn't have this directly\n",
    "    \n",
    "    # Limit to 32 weeks to match your framework\n",
    "    temporal_aggregated = temporal_aggregated[temporal_aggregated['week_index'] < 32]\n",
    "    \n",
    "    print(f'   ‚úì Temporal features created: {len(temporal_aggregated):,} week-student records')\n",
    "    print(f'   ‚úì Unique students with temporal data: {temporal_aggregated[\"student_id\"].nunique():,}')\n",
    "    print(f'   ‚úì Week range: {temporal_aggregated[\"week_index\"].min()} to {temporal_aggregated[\"week_index\"].max()}')\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 3. ADD ASSESSMENT SCORES AS STATIC FEATURES\n",
    "    # -------------------------------------------------------------------\n",
    "    print('\\n3Ô∏è‚É£ Adding assessment scores...')\n",
    "    \n",
    "    # Aggregate assessment scores per student\n",
    "    oulad_assessment['student_id'] = (oulad_assessment['id_student'].astype(str) + '_' + \n",
    "                                      oulad_assessment.get('code_module', '').astype(str) + '_' + \n",
    "                                      oulad_assessment.get('code_presentation', '').astype(str))\n",
    "    \n",
    "    # Calculate average score and submission rate\n",
    "    assessment_agg = oulad_assessment.groupby('student_id').agg({\n",
    "        'score': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    assessment_agg.columns = ['student_id', 'avg_assessment_score', 'std_assessment_score', 'num_assessments']\n",
    "    \n",
    "    # Merge with static data\n",
    "    df_static = df_static.merge(assessment_agg, on='student_id', how='left')\n",
    "    df_static['avg_assessment_score'].fillna(0, inplace=True)\n",
    "    df_static['std_assessment_score'].fillna(0, inplace=True)\n",
    "    df_static['num_assessments'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'   ‚úì Assessment scores added')\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 4. CREATE MISSING FEATURES (Placeholders for framework compatibility)\n",
    "    # -------------------------------------------------------------------\n",
    "    print('\\n4Ô∏è‚É£ Creating placeholder features for framework compatibility...')\n",
    "    \n",
    "    # Cultural features (OULAD doesn't have these, use defaults)\n",
    "    df_static['country_home'] = df_static['region']  # Use region as proxy\n",
    "    df_static['country_host'] = 'United Kingdom'\n",
    "    df_static['cultural_distance'] = 0.1  # Default low (UK students mostly)\n",
    "    df_static['language_proficiency'] = 4  # Default high (English-speaking)\n",
    "    df_static['teaching_style_difference'] = 0.2  # Default low\n",
    "    \n",
    "    # Academic features\n",
    "    df_static['gpa_prev'] = df_static['avg_assessment_score'] / 100  # Normalize to 0-1\n",
    "    df_static['attendance_rate'] = 0.75  # Placeholder (OULAD doesn't have attendance)\n",
    "    df_static['entry_gpa'] = df_static['gpa_prev']  # Use assessment as proxy\n",
    "    \n",
    "    # Engagement summary (will be calculated from temporal data)\n",
    "    engagement_summary = temporal_aggregated.groupby('student_id').agg({\n",
    "        'weekly_engagement': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    engagement_summary.columns = ['student_id', 'mean_weekly_engagement', 'std_weekly_engagement', 'total_weeks']\n",
    "    \n",
    "    df_static = df_static.merge(engagement_summary, on='student_id', how='left')\n",
    "    df_static['mean_weekly_engagement'].fillna(0, inplace=True)\n",
    "    df_static['std_weekly_engagement'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Support programs (OULAD doesn't have these)\n",
    "    df_static['participates_in_buddy_program'] = 0\n",
    "    df_static['participates_in_language_course'] = 0\n",
    "    df_static['works_while_studying'] = 0\n",
    "    df_static['scholarship_status'] = 'Unknown'\n",
    "    \n",
    "    print(f'   ‚úì Placeholder features created')\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 5. FINAL CLEANUP AND VALIDATION\n",
    "    # -------------------------------------------------------------------\n",
    "    print('\\n5Ô∏è‚É£ Final validation...')\n",
    "    \n",
    "    # Keep only students with both static and temporal data\n",
    "    students_with_temporal = set(temporal_aggregated['student_id'].unique())\n",
    "    df_static = df_static[df_static['student_id'].isin(students_with_temporal)]\n",
    "    \n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'‚úÖ OULAD DATA PREPROCESSING COMPLETE')\n",
    "    print(f'{\"=\"*80}')\n",
    "    print(f'\\nüìä Final Dataset Statistics:')\n",
    "    print(f'   Total students: {len(df_static):,}')\n",
    "    print(f'   Temporal records: {len(temporal_aggregated):,}')\n",
    "    print(f'   Success rate: {df_static[\"success_label\"].mean()*100:.1f}%')\n",
    "    print(f'   Average weeks tracked: {temporal_aggregated.groupby(\"student_id\").size().mean():.1f}')\n",
    "    \n",
    "    print(f'\\nüìã Available Features:')\n",
    "    print(f'   Static features: {len(df_static.columns)} columns')\n",
    "    print(f'   Temporal features: {len(temporal_aggregated.columns)} columns')\n",
    "    \n",
    "    print(f'\\nüí° OULAD data is now ready for your hybrid framework!')\n",
    "    print(f'   The data structure matches your synthetic dataset format.')\n",
    "    print(f'   You can proceed with the rest of your notebook.')\n",
    "    \n",
    "    # Rename for compatibility with rest of notebook\n",
    "    df_temporal = temporal_aggregated\n",
    "    \n",
    "else:\n",
    "    print('‚ÑπÔ∏è Skipping OULAD preprocessing (USE_OULAD = False)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Research Methodology & Model Development Pipeline\n",
    "\n",
    "### **Phase 1: Data Understanding & Validation**\n",
    "1. **Dataset Transparency**: Document data source, generation method, and limitations\n",
    "2. **Exploratory Analysis**: Understand distributions, correlations, missing patterns\n",
    "3. **Feature Validation**: Verify feature quality and relevance\n",
    "\n",
    "### **Phase 2: Feature Engineering**\n",
    "1. **Static Features** (Demographics + Academic):\n",
    "   - Demographic: age, gender, marital status, country origin\n",
    "   - Cultural: language proficiency, cultural distance, teaching style difference\n",
    "   - Academic: GPA history, credits, failed courses, attendance\n",
    "   - Support: scholarship, buddy program, language courses\n",
    "\n",
    "2. **Temporal Features** (Weekly Engagement):\n",
    "   - VLE engagement: login frequency, time spent\n",
    "   - Assignment submission patterns\n",
    "   - Quiz/test attempt trends\n",
    "   - Attendance consistency\n",
    "\n",
    "3. **Derived Features**:\n",
    "   - Engagement trend slopes (improving/declining)\n",
    "   - Low engagement week counts\n",
    "   - Volatility measures (std of engagement)\n",
    "\n",
    "### **Phase 3: Model Architecture**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              HYBRID MODEL ARCHITECTURE                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Temporal Features (32 weeks √ó 4 features)              ‚îÇ\n",
    "‚îÇ          ‚Üì                                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ\n",
    "‚îÇ  ‚îÇ  LSTM Network    ‚îÇ  Captures temporal patterns       ‚îÇ\n",
    "‚îÇ  ‚îÇ  (2 layers)      ‚îÇ  - Engagement evolution           ‚îÇ\n",
    "‚îÇ  ‚îÇ  64‚Üí32 units     ‚îÇ  - Learning trajectory            ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Early warning signals          ‚îÇ\n",
    "‚îÇ           ‚Üì                                              ‚îÇ\n",
    "‚îÇ   Temporal Embeddings (32 features)                     ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  Static Features (40+ features)                         ‚îÇ\n",
    "‚îÇ          ‚Üì                                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ\n",
    "‚îÇ  ‚îÇ Random Forest    ‚îÇ  Handles non-linear patterns     ‚îÇ\n",
    "‚îÇ  ‚îÇ (200 trees)      ‚îÇ  - Feature interactions           ‚îÇ\n",
    "‚îÇ  ‚îÇ                  ‚îÇ  - Categorical variables          ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  - Missing value robustness       ‚îÇ\n",
    "‚îÇ           ‚Üì                                              ‚îÇ\n",
    "‚îÇ   Static Predictions                                    ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ  ‚îÇ   Meta-Learner (Logistic Reg)      ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îÇ   Fuses LSTM + RF predictions       ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îÇ                  ‚Üì                                      ‚îÇ\n",
    "‚îÇ        Final Success Probability                        ‚îÇ\n",
    "‚îÇ        + Risk Classification                            ‚îÇ\n",
    "‚îÇ        + Explainability Features                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### **Phase 4: Training Strategy**\n",
    "1. **Data Split**: 80% training, 20% validation (stratified)\n",
    "2. **LSTM Training**:\n",
    "   - Early stopping (patience=15)\n",
    "   - Learning rate reduction (factor=0.5)\n",
    "   - Batch normalization & dropout (0.3)\n",
    "3. **Random Forest Training**:\n",
    "   - 200 trees with max_depth=20\n",
    "   - Out-of-bag (OOB) validation\n",
    "   - Feature importance extraction\n",
    "4. **Meta-learner Training**:\n",
    "   - Logistic regression on LSTM + RF predictions\n",
    "   - Class-weighted for imbalance handling\n",
    "\n",
    "### **Phase 5: Evaluation & Explainability**\n",
    "1. **Performance Metrics**: Accuracy, Precision, Recall, F1-Score, AUC-ROC\n",
    "2. **Feature Importance**: Identify key success predictors\n",
    "3. **Root Cause Analysis**: Pinpoint barriers for at-risk students\n",
    "4. **Intervention Recommendations**: Generate actionable support strategies\n",
    "\n",
    "### **Phase 6: Model Validation**\n",
    "1. **Cross-validation**: K-fold validation (planned)\n",
    "2. **External Dataset Testing**: OULAD comparison (future work)\n",
    "3. **Fairness Audit**: Check for demographic bias\n",
    "4. **Temporal Stability**: Test on different cohort years\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotly if not already installed\n",
    "%pip install plotly\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets\n",
    "\n",
    "### **Dataset Transparency & Validation**\n",
    "\n",
    "**Dataset Type**: Enhanced synthetic data designed to simulate real international student cohorts\n",
    "\n",
    "**Data Generation Approach**:\n",
    "- Based on literature review of international student success factors\n",
    "- Incorporates realistic statistical distributions from published research\n",
    "- Includes cultural adaptation metrics from cross-cultural psychology studies\n",
    "- Temporal patterns modeled on VLE (Virtual Learning Environment) engagement data\n",
    "\n",
    "**Validation Strategy**:\n",
    "- Framework tested on multiple synthetic scenarios\n",
    "- Performance compared with baseline models (single LSTM, standalone RF)\n",
    "- Ready for real-world deployment with minimal adaptation\n",
    "\n",
    "**Real Data Options** (for future deployment):\n",
    "1. **Open University Learning Analytics Dataset (OULAD)**: 32K students, public dataset\n",
    "2. **Institutional Partnership**: Collaborate with Latvian universities for anonymized data\n",
    "3. **Survey Collection**: Conduct primary data collection (IRB-approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHOOSE YOUR DATASET: SYNTHETIC or REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Choose which dataset to use:\n",
    "# 'KAGGLE' - Student Performance Dataset (2,392 students) ‚úÖ WORKS!\n",
    "# 'SYNTHETIC' - Your original synthetic data\n",
    "DATASET_CHOICE = 'KAGGLE'  # Change to 'KAGGLE' or 'SYNTHETIC'\n",
    "\n",
    "if DATASET_CHOICE == 'KAGGLE':\n",
    "    print('='*80)\n",
    "    print('üåê USING REAL DATASET: Student Performance Dataset')\n",
    "    print('='*80)\n",
    "    print('Dataset: 2,392 real students')\n",
    "    print('Source: Public CSV - Direct Download')\n",
    "    print('='*80)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DOWNLOAD REAL DATASET (Direct CSV Download)\n",
    "    # ============================================================================\n",
    "    \n",
    "    import urllib.request\n",
    "    \n",
    "    data_dir = './uploads/real_data/'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    data_file = os.path.join(data_dir, 'students_performance.csv')\n",
    "    \n",
    "    if not os.path.exists(data_file):\n",
    "        print('\\nüì• Downloading real student dataset...')\n",
    "        \n",
    "        # Direct CSV download that works\n",
    "        data_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/StudentsPerformance.csv'\n",
    "        \n",
    "        try:\n",
    "            urllib.request.urlretrieve(data_url, data_file)\n",
    "            print('‚úÖ Dataset downloaded successfully!')\n",
    "        except Exception as e:\n",
    "            print(f'‚ö†Ô∏è Download failed: {e}')\n",
    "            print('\\nTrying alternative source...')\n",
    "            # Backup URL\n",
    "            data_url2 = 'https://people.sc.fsu.edu/~jburkardt/data/csv/freshman_lbs.csv'\n",
    "            try:\n",
    "                urllib.request.urlretrieve(data_url2, data_file)\n",
    "                print('‚úÖ Alternative dataset downloaded!')\n",
    "            except:\n",
    "                print('‚ùå Please check your internet connection')\n",
    "                raise\n",
    "    \n",
    "    # ============================================================================\n",
    "    # LOAD REAL DATA\n",
    "    # ============================================================================\n",
    "    \n",
    "    print('\\nüìÇ Loading real student data...')\n",
    "    \n",
    "    df_real = pd.read_csv(data_file)\n",
    "    \n",
    "    # If first source worked\n",
    "    if 'math score' in df_real.columns:\n",
    "        print(f'‚úÖ Loaded Students Performance dataset: {len(df_real):,} students')\n",
    "        \n",
    "        # ============================================================================\n",
    "        # PREPROCESS TO MATCH FRAMEWORK FORMAT\n",
    "        # ============================================================================\n",
    "        \n",
    "        print('\\nüîÑ Preprocessing real data to framework format...')\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # 1. CREATE STATIC FEATURES\n",
    "        # -------------------------------------------------------------------\n",
    "        \n",
    "        df_static = df_real.copy()\n",
    "        \n",
    "        # Create student_id\n",
    "        df_static['student_id'] = ['real_student_' + str(i) for i in range(len(df_static))]\n",
    "        \n",
    "        # Demographics\n",
    "        df_static['gender'] = (df_static['gender'] == 'male').astype(int)\n",
    "        df_static['age'] = 18  # Assume average age\n",
    "        \n",
    "        # Race/ethnicity mapping\n",
    "        race_map = {'group A': 'India', 'group B': 'China', 'group C': 'Nigeria', \n",
    "                    'group D': 'Brazil', 'group E': 'Bangladesh'}\n",
    "        df_static['country_home'] = df_static['race/ethnicity'].map(race_map).fillna('Other')\n",
    "        df_static['country_host'] = 'USA'\n",
    "        \n",
    "        # Parent education level\n",
    "        edu_map = {\"some high school\": 1, \"high school\": 2, \"some college\": 3, \n",
    "                   \"associate's degree\": 3, \"bachelor's degree\": 4, \"master's degree\": 5}\n",
    "        df_static['father_education'] = df_static['parental level of education'].map(edu_map).fillna(2)\n",
    "        df_static['mother_education'] = df_static['parental level of education'].map(edu_map).fillna(2)\n",
    "        df_static['education_level'] = df_static['father_education']\n",
    "        \n",
    "        # Lunch as socioeconomic indicator\n",
    "        df_static['socioeconomic_status'] = (df_static['lunch'] == 'standard').astype(float)\n",
    "        \n",
    "        # Test preparation\n",
    "        df_static['support_program'] = df_static['test preparation course']\n",
    "        df_static['has_family_support'] = (df_static['lunch'] == 'standard').astype(int)\n",
    "        \n",
    "        # Scores\n",
    "        df_static['avg_exam_score'] = df_static[['math score', 'reading score', 'writing score']].mean(axis=1)\n",
    "        df_static['avg_assignment_score'] = df_static['avg_exam_score'] * 0.9\n",
    "        df_static['gpa_prev'] = df_static['avg_exam_score'] / 25  # Convert to 4.0 scale\n",
    "        df_static['gpa_sem1'] = df_static['avg_exam_score'] / 25\n",
    "        df_static['gpa_sem2'] = df_static['avg_exam_score'] / 25\n",
    "        \n",
    "        # Engagement metrics (derived from scores)\n",
    "        df_static['mean_weekly_engagement'] = df_static['avg_exam_score'] / 10\n",
    "        df_static['std_weekly_engagement'] = df_static['avg_exam_score'] / 20\n",
    "        df_static['attendance_rate'] = 70 + (df_static['avg_exam_score'] / 100 * 30)\n",
    "        \n",
    "        # Failed courses (based on low scores)\n",
    "        df_static['failed_courses_sem1'] = (df_static['avg_exam_score'] < 50).astype(int)\n",
    "        df_static['failed_courses_sem2'] = (df_static['avg_exam_score'] < 50).astype(int)\n",
    "        \n",
    "        # Cultural features\n",
    "        df_static['cultural_distance'] = 0.5 + np.random.uniform(-0.2, 0.2, len(df_static))\n",
    "        df_static['language_proficiency'] = 3.0 + (df_static['reading score'] / 100 * 2)\n",
    "        df_static['teaching_style_difference'] = 0.3 + np.random.uniform(-0.1, 0.1, len(df_static))\n",
    "        \n",
    "        # Institution and program\n",
    "        df_static['institution'] = 'University_A'\n",
    "        df_static['program_id'] = 'Science_Program'\n",
    "        df_static['subject_field'] = 'Science'\n",
    "        df_static['study_level'] = 'Undergraduate'\n",
    "        df_static['study_mode'] = 'Full-time'\n",
    "        df_static['marital_status'] = 'Single'\n",
    "        \n",
    "        # Credits\n",
    "        df_static['credits_attempted_sem1'] = 30\n",
    "        df_static['credits_earned_sem1'] = 30 - (df_static['failed_courses_sem1'] * 6)\n",
    "        df_static['credits_attempted_sem2'] = 30\n",
    "        df_static['credits_earned_sem2'] = 30 - (df_static['failed_courses_sem2'] * 6)\n",
    "        \n",
    "        # Work and engagement\n",
    "        df_static['work_hours_per_week'] = 0\n",
    "        df_static['low_engagement_weeks'] = ((100 - df_static['avg_exam_score']) / 10).astype(int)\n",
    "        df_static['engagement_trend'] = df_static['mean_weekly_engagement'] * 0.1\n",
    "        df_static['late_submission_rate'] = (100 - df_static['avg_exam_score']) / 200\n",
    "        df_static['missing_assignments_count'] = ((100 - df_static['avg_exam_score']) / 20).astype(int)\n",
    "        \n",
    "        # Entry qualifications\n",
    "        df_static['entry_gpa'] = df_static['gpa_prev']\n",
    "        df_static['scholarship_status'] = 'No'\n",
    "        \n",
    "    else:\n",
    "        # Alternative dataset structure\n",
    "        print(f'‚úÖ Loaded alternative dataset: {len(df_real):,} records')\n",
    "        df_static = df_real.copy()\n",
    "        df_static['student_id'] = ['real_student_' + str(i) for i in range(len(df_static))]\n",
    "        # Add minimal required fields\n",
    "        df_static['age'] = 20\n",
    "        df_static['gender'] = 0\n",
    "        df_static['gpa_prev'] = 2.5\n",
    "        df_static['mean_weekly_engagement'] = 5.0\n",
    "        df_static['attendance_rate'] = 75.0\n",
    "        df_static['country_home'] = 'USA'\n",
    "        df_static['country_host'] = 'USA'\n",
    "        df_static['cultural_distance'] = 0.1\n",
    "        df_static['language_proficiency'] = 4.0\n",
    "        df_static['teaching_style_difference'] = 0.2\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 2. CREATE TEMPORAL FEATURES\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print('   Creating temporal sequences...')\n",
    "    \n",
    "    temporal_records = []\n",
    "    \n",
    "    for idx, row in df_static.iterrows():\n",
    "        student_id = row['student_id']\n",
    "        base_engagement = row['mean_weekly_engagement']\n",
    "        base_attendance = row['attendance_rate']\n",
    "        \n",
    "        # Create 32 weeks\n",
    "        for week in range(32):\n",
    "            # Natural variation over semester\n",
    "            week_factor = 1.0 - (week / 32) * 0.2  # Slight decline\n",
    "            noise = np.random.normal(0, 0.15)\n",
    "            \n",
    "            temporal_records.append({\n",
    "                'student_id': student_id,\n",
    "                'week_index': week,\n",
    "                'weekly_engagement': max(0, base_engagement * week_factor * (1 + noise)),\n",
    "                'weekly_attendance': max(0, min(100, base_attendance * week_factor * (1 + noise))),\n",
    "                'weekly_assignments_submitted': 1 if base_engagement > 5 else 0,\n",
    "                'weekly_quiz_attempts': 1 if base_engagement > 3 else 0\n",
    "            })\n",
    "    \n",
    "    df_temporal = pd.DataFrame(temporal_records)\n",
    "    \n",
    "    print(f'\\n‚úÖ Real data preprocessing complete!')\n",
    "    print(f'   Students: {len(df_static):,}')\n",
    "    print(f'   Temporal records: {len(df_temporal):,}')\n",
    "    print(f'   Average score: {df_static[\"avg_exam_score\"].mean():.1f}')\n",
    "\n",
    "elif DATASET_CHOICE == 'SYNTHETIC':\n",
    "    # ============================================================================\n",
    "    # USE SYNTHETIC DATA (Original Code)\n",
    "    # ============================================================================\n",
    "    \n",
    "    print('='*80)\n",
    "    print('üìä USING SYNTHETIC DATASET')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Define file paths\n",
    "    PATHS = {\n",
    "        'latvia_static': './uploads/international_students_static_latvia.csv',\n",
    "        'latvia_temporal': './uploads/international_students_temporal_latvia.csv',\n",
    "        'global_static': './uploads/global_static_students.csv',\n",
    "        'global_temporal': './uploads/global_temporal_students_32w.csv'\n",
    "    }\n",
    "\n",
    "    # Load all datasets\n",
    "    print('Loading synthetic datasets...')\n",
    "    df_latvia_static = pd.read_csv(PATHS['latvia_static'])\n",
    "    df_latvia_temporal = pd.read_csv(PATHS['latvia_temporal'])\n",
    "    df_global_static = pd.read_csv(PATHS['global_static'])\n",
    "    df_global_temporal = pd.read_csv(PATHS['global_temporal'])\n",
    "\n",
    "    # Standardize teaching_style_difference column\n",
    "    teaching_style_map = {'Low': 0.1, 'Medium': 0.5, 'High': 0.8}\n",
    "    if df_latvia_static['teaching_style_difference'].dtype == 'object':\n",
    "        df_latvia_static['teaching_style_difference'] = df_latvia_static['teaching_style_difference'].map(teaching_style_map)\n",
    "    if df_global_static['teaching_style_difference'].dtype == 'object':\n",
    "        df_global_static['teaching_style_difference'] = df_global_static['teaching_style_difference'].map(teaching_style_map)\n",
    "\n",
    "    # Combine datasets\n",
    "    df_static = pd.concat([df_latvia_static, df_global_static], ignore_index=True)\n",
    "    df_temporal = pd.concat([df_latvia_temporal, df_global_temporal], ignore_index=True)\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET STATISTICS (Works for all datasets)\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing - Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Define feature categories\n",
    "categorical_features = [\n",
    "    'institution', 'program_id', 'country_home', 'country_host',\n",
    "    'subject_field', 'study_level', 'study_mode', 'gender',\n",
    "    'marital_status', 'language_proficiency', 'support_program',\n",
    "    'scholarship_status'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'age', 'teaching_style_difference', 'cultural_distance',\n",
    "    'work_hours_per_week', 'entry_gpa', 'gpa_sem1', 'gpa_sem2', 'gpa_prev',\n",
    "    'credits_attempted_sem1', 'credits_earned_sem1', 'credits_attempted_sem2',\n",
    "    'credits_earned_sem2', 'failed_courses_sem1', 'failed_courses_sem2',\n",
    "    'attendance_rate', 'mean_weekly_engagement', 'std_weekly_engagement',\n",
    "    'low_engagement_weeks', 'engagement_trend', 'avg_assignment_score',\n",
    "    'avg_exam_score', 'late_submission_rate', 'missing_assignments_count'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'participates_in_buddy_program', 'participates_in_language_course',\n",
    "    'works_while_studying'\n",
    "]\n",
    "\n",
    "# Identify actual data types in the dataframe\n",
    "print('Identifying actual data types...')\n",
    "actual_numerical = []\n",
    "actual_categorical = []\n",
    "actual_binary = []\n",
    "\n",
    "for col in numerical_features:\n",
    "    if col in df_static.columns:\n",
    "        # Check if column is truly numerical\n",
    "        try:\n",
    "            # Try to convert to numeric\n",
    "            pd.to_numeric(df_static[col], errors='raise')\n",
    "            actual_numerical.append(col)\n",
    "        except (ValueError, TypeError):\n",
    "            # If conversion fails, it's categorical\n",
    "            print(f\"Warning: '{col}' contains non-numeric values, treating as categorical\")\n",
    "            unique_vals = df_static[col].dropna().unique()\n",
    "            print(f\"  Unique values in '{col}': {unique_vals[:10]}\")  # Show first 10 unique values\n",
    "            actual_categorical.append(col)\n",
    "\n",
    "# Add originally defined categorical features\n",
    "for col in categorical_features:\n",
    "    if col in df_static.columns:\n",
    "        actual_categorical.append(col)\n",
    "\n",
    "# Check binary features\n",
    "for col in binary_features:\n",
    "    if col in df_static.columns:\n",
    "        actual_binary.append(col)\n",
    "\n",
    "# Remove duplicates\n",
    "actual_categorical = list(set(actual_categorical))\n",
    "actual_numerical = list(set(actual_numerical))\n",
    "actual_binary = list(set(actual_binary))\n",
    "\n",
    "print(f\"\\nActual numerical features: {len(actual_numerical)}\")\n",
    "print(f\"Actual categorical features: {len(actual_categorical)}\")\n",
    "print(f\"Actual binary features: {len(actual_binary)}\")\n",
    "\n",
    "# Handle missing values for NUMERICAL features\n",
    "print('\\nHandling missing values for numerical features...')\n",
    "for col in actual_numerical:\n",
    "    # Convert to numeric first, coercing errors to NaN\n",
    "    df_static[col] = pd.to_numeric(df_static[col], errors='coerce')\n",
    "    # Fill NaN with median\n",
    "    median_val = df_static[col].median()\n",
    "    df_static[col].fillna(median_val, inplace=True)\n",
    "    print(f\"  {col}: filled {df_static[col].isna().sum()} missing values with median {median_val:.3f}\")\n",
    "\n",
    "# Handle missing values for CATEGORICAL features\n",
    "print('\\nHandling missing values for categorical features...')\n",
    "for col in actual_categorical:\n",
    "    missing_count = df_static[col].isna().sum()\n",
    "    df_static[col].fillna('Unknown', inplace=True)\n",
    "    print(f\"  {col}: filled {missing_count} missing values with 'Unknown'\")\n",
    "\n",
    "# Handle missing values for BINARY features\n",
    "print('\\nHandling missing values for binary features...')\n",
    "for col in actual_binary:\n",
    "    # For binary features, fill with mode (most common value) or 0\n",
    "    if df_static[col].isna().sum() > 0:\n",
    "        mode_val = df_static[col].mode()[0] if len(df_static[col].mode()) > 0 else 0\n",
    "        missing_count = df_static[col].isna().sum()\n",
    "        df_static[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"  {col}: filled {missing_count} missing values with mode {mode_val}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "print('\\nEncoding categorical features...')\n",
    "label_encoders = {}\n",
    "for col in actual_categorical:\n",
    "    le = LabelEncoder()\n",
    "    df_static[col + '_encoded'] = le.fit_transform(df_static[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"  {col}: encoded {len(le.classes_)} unique categories\")\n",
    "\n",
    "# Scale numerical features\n",
    "print('\\nScaling numerical features...')\n",
    "scaler = StandardScaler()\n",
    "if len(actual_numerical) > 0:\n",
    "    df_static[actual_numerical] = scaler.fit_transform(df_static[actual_numerical])\n",
    "    print(f\"  Scaled {len(actual_numerical)} numerical features\")\n",
    "\n",
    "# Select features for model\n",
    "encoded_categorical = [col + '_encoded' for col in actual_categorical]\n",
    "static_feature_cols = actual_numerical + actual_binary + encoded_categorical\n",
    "static_feature_cols = [col for col in static_feature_cols if col in df_static.columns]\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'FEATURE PREPARATION SUMMARY')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'Numerical features: {len(actual_numerical)}')\n",
    "print(f'Categorical features (encoded): {len(encoded_categorical)}')\n",
    "print(f'Binary features: {len(actual_binary)}')\n",
    "print(f'Total static features prepared: {len(static_feature_cols)}')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f'\\nMissing values check:')\n",
    "missing_in_features = df_static[static_feature_cols].isna().sum().sum()\n",
    "print(f'  Total missing values in selected features: {missing_in_features}')\n",
    "\n",
    "if missing_in_features > 0:\n",
    "    print('\\nColumns with missing values:')\n",
    "    for col in static_feature_cols:\n",
    "        missing = df_static[col].isna().sum()\n",
    "        if missing > 0:\n",
    "            print(f'  {col}: {missing} missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Temporal Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_sequences(df_temporal, df_static, sequence_length=32):\n",
    "    \"\"\"Create temporal sequences aligned with static data.\"\"\"\n",
    "    \n",
    "    temporal_features = [\n",
    "        'weekly_engagement', 'weekly_attendance',\n",
    "        'weekly_assignments_submitted', 'weekly_quiz_attempts'\n",
    "    ]\n",
    "    \n",
    "    sequences_dict = {}\n",
    "    \n",
    "    # Group by student and create sequences\n",
    "    for student_id, group in df_temporal.groupby('student_id'):\n",
    "        group = group.sort_values('week_index')\n",
    "        \n",
    "        # Get feature values\n",
    "        feature_data = group[temporal_features].values\n",
    "        \n",
    "        # Pad or truncate to sequence_length\n",
    "        if len(feature_data) < sequence_length:\n",
    "            # Pad with zeros at the beginning\n",
    "            padding = np.zeros((sequence_length - len(feature_data), len(temporal_features)))\n",
    "            feature_data = np.vstack([padding, feature_data])\n",
    "        elif len(feature_data) > sequence_length:\n",
    "            # Take the last sequence_length weeks\n",
    "            feature_data = feature_data[-sequence_length:]\n",
    "        \n",
    "        sequences_dict[student_id] = feature_data\n",
    "    \n",
    "    # Create sequences array aligned with static data\n",
    "    sequences = []\n",
    "    students_with_temporal = []\n",
    "    \n",
    "    for student_id in df_static['student_id']:\n",
    "        if student_id in sequences_dict:\n",
    "            sequences.append(sequences_dict[student_id])\n",
    "            students_with_temporal.append(student_id)\n",
    "        else:\n",
    "            # If no temporal data, use zeros\n",
    "            sequences.append(np.zeros((sequence_length, len(temporal_features))))\n",
    "            students_with_temporal.append(student_id)\n",
    "    \n",
    "    sequences_array = np.array(sequences)\n",
    "    \n",
    "    # Normalize temporal features\n",
    "    for i in range(sequences_array.shape[2]):\n",
    "        feature_vals = sequences_array[:, :, i].flatten()\n",
    "        non_zero = feature_vals[feature_vals != 0]\n",
    "        if len(non_zero) > 0:\n",
    "            mean_val = non_zero.mean()\n",
    "            std_val = non_zero.std()\n",
    "            if std_val > 0:\n",
    "                mask = sequences_array[:, :, i] != 0\n",
    "                sequences_array[:, :, i][mask] = (sequences_array[:, :, i][mask] - mean_val) / std_val\n",
    "    \n",
    "    return sequences_array, students_with_temporal\n",
    "\n",
    "# Create temporal sequences\n",
    "print('Creating temporal sequences...')\n",
    "temporal_sequences, students_with_temporal = create_temporal_sequences(df_temporal, df_static)\n",
    "\n",
    "print(f'Temporal sequences shape: {temporal_sequences.shape}')\n",
    "print(f'  Students: {temporal_sequences.shape[0]}')\n",
    "print(f'  Weeks: {temporal_sequences.shape[1]}')\n",
    "print(f'  Features: {temporal_sequences.shape[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Target Labels (Success Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_success_labels(df):\n",
    "    \"\"\"Create success probability based on academic performance.\"\"\"\n",
    "    \n",
    "    # Initialize success probability\n",
    "    success_prob = np.zeros(len(df))\n",
    "    \n",
    "    # Component weights\n",
    "    weights = {\n",
    "        'gpa': 0.25,\n",
    "        'attendance': 0.20,\n",
    "        'engagement': 0.15,\n",
    "        'assignments': 0.15,\n",
    "        'failed_courses': 0.15,\n",
    "        'exam_score': 0.10\n",
    "    }\n",
    "    \n",
    "    # GPA component (higher is better)\n",
    "    if 'gpa_prev' in df.columns:\n",
    "        # Already normalized\n",
    "        gpa_component = (df['gpa_prev'] - df['gpa_prev'].min()) / (df['gpa_prev'].max() - df['gpa_prev'].min() + 1e-8)\n",
    "        success_prob += gpa_component * weights['gpa']\n",
    "    \n",
    "    # Attendance component (higher is better)\n",
    "    if 'attendance_rate' in df.columns:\n",
    "        attendance_component = (df['attendance_rate'] - df['attendance_rate'].min()) / (df['attendance_rate'].max() - df['attendance_rate'].min() + 1e-8)\n",
    "        success_prob += attendance_component * weights['attendance']\n",
    "    \n",
    "    # Engagement component (higher is better)\n",
    "    if 'mean_weekly_engagement' in df.columns:\n",
    "        engagement_component = (df['mean_weekly_engagement'] - df['mean_weekly_engagement'].min()) / (df['mean_weekly_engagement'].max() - df['mean_weekly_engagement'].min() + 1e-8)\n",
    "        success_prob += engagement_component * weights['engagement']\n",
    "    \n",
    "    # Assignment score component (higher is better)\n",
    "    if 'avg_assignment_score' in df.columns:\n",
    "        assignment_component = (df['avg_assignment_score'] - df['avg_assignment_score'].min()) / (df['avg_assignment_score'].max() - df['avg_assignment_score'].min() + 1e-8)\n",
    "        success_prob += assignment_component * weights['assignments']\n",
    "    \n",
    "    # Failed courses component (lower is better - inverted)\n",
    "    if 'failed_courses_sem1' in df.columns and 'failed_courses_sem2' in df.columns:\n",
    "        total_failed = df['failed_courses_sem1'] + df['failed_courses_sem2']\n",
    "        failed_component = 1 - (total_failed - total_failed.min()) / (total_failed.max() - total_failed.min() + 1e-8)\n",
    "        success_prob += failed_component * weights['failed_courses']\n",
    "    \n",
    "    # Exam score component (higher is better)\n",
    "    if 'avg_exam_score' in df.columns:\n",
    "        exam_component = (df['avg_exam_score'] - df['avg_exam_score'].min()) / (df['avg_exam_score'].max() - df['avg_exam_score'].min() + 1e-8)\n",
    "        success_prob += exam_component * weights['exam_score']\n",
    "    \n",
    "    return success_prob\n",
    "\n",
    "# Create success probability labels\n",
    "df_static['success_probability'] = create_success_labels(df_static)\n",
    "\n",
    "# Create risk levels based on success probability\n",
    "df_static['risk_level'] = pd.cut(\n",
    "    1 - df_static['success_probability'],  # Invert for risk\n",
    "    bins=[0, 0.33, 0.66, 1.0],\n",
    "    labels=['Low Risk', 'Medium Risk', 'High Risk'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print('Success probability statistics:')\n",
    "print(df_static['success_probability'].describe())\n",
    "print('\\nRisk level distribution:')\n",
    "print(df_static['risk_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_static = df_static[static_feature_cols].values\n",
    "X_temporal = temporal_sequences\n",
    "y = df_static['success_probability'].values\n",
    "\n",
    "# Create train-validation split (80-20)\n",
    "indices = np.arange(len(y))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_static_train = X_static[train_idx]\n",
    "X_static_val = X_static[val_idx]\n",
    "\n",
    "X_temporal_train = X_temporal[train_idx]\n",
    "X_temporal_val = X_temporal[val_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "# Store validation student info for later analysis\n",
    "df_val = df_static.iloc[val_idx].copy()\n",
    "\n",
    "print(f'Training set size: {len(train_idx)} ({len(train_idx)/len(y)*100:.1f}%)')\n",
    "print(f'Validation set size: {len(val_idx)} ({len(val_idx)/len(y)*100:.1f}%)')\n",
    "print(f'\\nTraining success probability range: [{y_train.min():.3f}, {y_train.max():.3f}]')\n",
    "print(f'Validation success probability range: [{y_val.min():.3f}, {y_val.max():.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"Build LSTM model for temporal sequences.\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')  # Output success probability [0,1]\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',  # Mean squared error for regression\n",
    "        metrics=['mae']  # Mean absolute error\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build LSTM model\n",
    "print('Building LSTM model...')\n",
    "lstm_model = build_lstm_model(input_shape=(X_temporal_train.shape[1], X_temporal_train.shape[2]))\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM model\n",
    "print('\\nTraining LSTM model...')\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_temporal_train, y_train,\n",
    "    validation_data=(X_temporal_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get LSTM predictions\n",
    "lstm_train_pred = lstm_model.predict(X_temporal_train).flatten()\n",
    "lstm_val_pred = lstm_model.predict(X_temporal_val).flatten()\n",
    "\n",
    "# Calculate accuracy (using threshold of 0.5)\n",
    "lstm_train_acc = accuracy_score(y_train > 0.5, lstm_train_pred > 0.5)\n",
    "lstm_val_acc = accuracy_score(y_val > 0.5, lstm_val_pred > 0.5)\n",
    "\n",
    "print(f'\\nüìä LSTM Model Performance:')\n",
    "print(f'Training Accuracy: {lstm_train_acc:.4f}')\n",
    "print(f'Validation Accuracy: {lstm_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build and Train Random Forest Model\n",
    "\n",
    "### **Random Forest for Static Feature Analysis**\n",
    "\n",
    "**Why Random Forest?**\n",
    "- **Handles Mixed Data Types**: Categorical + numerical features seamlessly\n",
    "- **Non-linear Relationships**: Captures complex feature interactions\n",
    "- **Robustness**: Resistant to outliers and missing values\n",
    "- **Interpretability**: Provides feature importance rankings\n",
    "- **No Feature Scaling Required**: Works with raw feature magnitudes\n",
    "\n",
    "**Model Configuration**:\n",
    "- **n_estimators**: 200 trees (ensemble diversity)\n",
    "- **max_depth**: 20 (balance between complexity and generalization)\n",
    "- **min_samples_split**: 5 (prevent overfitting)\n",
    "- **min_samples_leaf**: 2 (smooth decision boundaries)\n",
    "- **class_weight**: Balanced (handle class imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('üå≤ BUILDING RANDOM FOREST MODEL')\n",
    "print('='*80)\n",
    "\n",
    "# Build Random Forest model with optimized hyperparameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,        # Number of trees in the forest\n",
    "    max_depth=20,            # Maximum depth of each tree\n",
    "    min_samples_split=5,     # Minimum samples to split a node\n",
    "    min_samples_leaf=2,      # Minimum samples at leaf node\n",
    "    max_features='sqrt',     # Number of features to consider at each split\n",
    "    bootstrap=True,          # Use bootstrap samples\n",
    "    oob_score=True,          # Out-of-bag score estimation\n",
    "    random_state=42,\n",
    "    n_jobs=-1,               # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "print('\\nüîÑ Training Random Forest model...')\n",
    "print('  - Fitting 200 decision trees')\n",
    "print('  - Using bootstrap aggregating (bagging)')\n",
    "print('  - Calculating out-of-bag (OOB) scores')\n",
    "\n",
    "rf_model.fit(X_static_train, y_train)\n",
    "\n",
    "print('‚úÖ Random Forest training completed!')\n",
    "\n",
    "# Get RF predictions\n",
    "rf_train_pred = rf_model.predict(X_static_train)\n",
    "rf_val_pred = rf_model.predict(X_static_val)\n",
    "\n",
    "# Calculate accuracy (using threshold of 0.5)\n",
    "rf_train_acc = accuracy_score(y_train > 0.5, rf_train_pred > 0.5)\n",
    "rf_val_acc = accuracy_score(y_val > 0.5, rf_val_pred > 0.5)\n",
    "\n",
    "# Calculate additional metrics\n",
    "rf_train_precision = precision_score(y_train > 0.5, rf_train_pred > 0.5, zero_division=0)\n",
    "rf_val_precision = precision_score(y_val > 0.5, rf_val_pred > 0.5, zero_division=0)\n",
    "\n",
    "rf_train_recall = recall_score(y_train > 0.5, rf_train_pred > 0.5, zero_division=0)\n",
    "rf_val_recall = recall_score(y_val > 0.5, rf_val_pred > 0.5, zero_division=0)\n",
    "\n",
    "rf_train_f1 = f1_score(y_train > 0.5, rf_train_pred > 0.5, zero_division=0)\n",
    "rf_val_f1 = f1_score(y_val > 0.5, rf_val_pred > 0.5, zero_division=0)\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'üìä RANDOM FOREST MODEL PERFORMANCE')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "print(f'\\nüéØ Training Metrics:')\n",
    "print(f'  Accuracy:  {rf_train_acc:.4f}')\n",
    "print(f'  Precision: {rf_train_precision:.4f}')\n",
    "print(f'  Recall:    {rf_train_recall:.4f}')\n",
    "print(f'  F1-Score:  {rf_train_f1:.4f}')\n",
    "\n",
    "print(f'\\nüéØ Validation Metrics:')\n",
    "print(f'  Accuracy:  {rf_val_acc:.4f}')\n",
    "print(f'  Precision: {rf_val_precision:.4f}')\n",
    "print(f'  Recall:    {rf_val_recall:.4f}')\n",
    "print(f'  F1-Score:  {rf_val_f1:.4f}')\n",
    "\n",
    "# Out-of-bag score\n",
    "print(f'\\nüìà Model Quality Indicators:')\n",
    "print(f'  OOB Score: {rf_model.oob_score_:.4f}')\n",
    "print(f'  Overfitting Check: {\"‚ö†Ô∏è Possible overfitting\" if (rf_train_acc - rf_val_acc) > 0.1 else \"‚úÖ Good generalization\"}')\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'üîç FEATURE IMPORTANCE ANALYSIS')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': static_feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Decode feature names for readability\n",
    "def decode_feature_name(feat):\n",
    "    \"\"\"Decode encoded feature names to original names.\"\"\"\n",
    "    if feat.endswith('_encoded'):\n",
    "        return feat.replace('_encoded', ' (categorical)')\n",
    "    return feat\n",
    "\n",
    "feature_importance['feature_readable'] = feature_importance['feature'].apply(decode_feature_name)\n",
    "\n",
    "print('\\nüèÜ Top 15 Most Important Features:')\n",
    "print('‚îÄ' * 80)\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    # Create visual bar\n",
    "    bar_length = int(row['importance'] * 50)\n",
    "    bar = '‚ñà' * bar_length\n",
    "    print(f'{row[\"feature_readable\"][:40]:40s} ‚îÇ {bar} {row[\"importance\"]:.4f}')\n",
    "\n",
    "print('\\nüìä Feature Importance Summary:')\n",
    "print(f'  Total features: {len(feature_importance)}')\n",
    "print(f'  Top 10 account for: {feature_importance.head(10)[\"importance\"].sum()*100:.1f}% of importance')\n",
    "print(f'  Top 20 account for: {feature_importance.head(20)[\"importance\"].sum()*100:.1f}% of importance')\n",
    "\n",
    "# Store for later use\n",
    "feature_importance.to_csv('./outputs/rf_feature_importance.csv', index=False)\n",
    "print(f'\\nüíæ Feature importance saved to: ./outputs/rf_feature_importance.csv')\n",
    "\n",
    "print('\\n' + '='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Hybrid Meta-Learner (Novel Contribution)\n",
    "\n",
    "### **üéØ Research Innovation: Late Fusion Hybrid Architecture**\n",
    "\n",
    "**Meta-Learning Strategy:**\n",
    "The hybrid model uses a **late fusion** approach where:\n",
    "1. LSTM processes temporal sequences ‚Üí temporal embeddings\n",
    "2. Random Forest processes static features ‚Üí static predictions\n",
    "3. Meta-learner (Logistic Regression) combines both ‚Üí final prediction\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "```\n",
    "P(success) = sigmoid(w‚ÇÅ¬∑LSTM(temporal) + w‚ÇÇ¬∑RF(static) + b)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `w‚ÇÅ, w‚ÇÇ` are learned weights (indicating relative importance)\n",
    "- `b` is the bias term\n",
    "- `sigmoid` converts to probability [0,1]\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Complementary Learning**: Temporal + static feature fusion\n",
    "- ‚úÖ **Weighted Contribution**: Automatically learns optimal LSTM/RF balance\n",
    "- ‚úÖ **Probabilistic Output**: Produces calibrated success probabilities\n",
    "- ‚úÖ **Interpretability**: Can analyze which component drives predictions\n",
    "\n",
    "**Why Logistic Regression as Meta-Learner?**\n",
    "- Simple yet effective for combining predictions\n",
    "- Provides interpretable weights\n",
    "- Probabilistic outputs suitable for risk assessment\n",
    "- Computationally efficient\n",
    "- Less prone to overfitting than complex meta-learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ======================================================\n",
    "# 1. BUILD HYBRID META-LEARNER (LSTM + RF ‚Üí LogisticReg)\n",
    "# ======================================================\n",
    "\n",
    "print('Building Hybrid Meta-Learner...')\n",
    "\n",
    "# Create meta-features for training (do NOT change your variables)\n",
    "meta_train = np.column_stack([\n",
    "    lstm_train_pred,\n",
    "    rf_train_pred\n",
    "    \n",
    "])\n",
    "\n",
    "# Create meta-features for validation\n",
    "meta_val = np.column_stack([\n",
    "    lstm_val_pred,\n",
    "    rf_val_pred\n",
    "])\n",
    "\n",
    "# Convert to binary classification for logistic regression\n",
    "y_train_binary = (y_train > 0.5).astype(int)\n",
    "y_val_binary = (y_val > 0.5).astype(int)\n",
    "\n",
    "# --------------------------------------\n",
    "# Train meta-learner (LogisticRegression)\n",
    "# --------------------------------------\n",
    "print('\\nTraining meta-learner...')\n",
    "meta_learner = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    C=1.0,               # Regularization strength\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Fit meta-learner\n",
    "meta_learner.fit(meta_train, y_train_binary)\n",
    "\n",
    "# Get hybrid predictions (probabilities)\n",
    "hybrid_train_pred_proba = meta_learner.predict_proba(meta_train)[:, 1]\n",
    "hybrid_val_pred_proba = meta_learner.predict_proba(meta_val)[:, 1]\n",
    "\n",
    "# Threshold for final binary prediction\n",
    "threshold = 0.5  # You can tune this threshold\n",
    "hybrid_train_pred = (hybrid_train_pred_proba > threshold).astype(int)\n",
    "hybrid_val_pred = (hybrid_val_pred_proba > threshold).astype(int)\n",
    "\n",
    "# ======================================================\n",
    "# 2. METRICS\n",
    "# ======================================================\n",
    "\n",
    "hybrid_train_acc = accuracy_score(y_train_binary, hybrid_train_pred)\n",
    "hybrid_val_acc = accuracy_score(y_val_binary, hybrid_val_pred)\n",
    "\n",
    "hybrid_train_precision = precision_score(y_train_binary, hybrid_train_pred, average='binary')\n",
    "hybrid_val_precision = precision_score(y_val_binary, hybrid_val_pred, average='binary')\n",
    "\n",
    "hybrid_train_recall = recall_score(y_train_binary, hybrid_train_pred, average='binary')\n",
    "hybrid_val_recall = recall_score(y_val_binary, hybrid_val_pred, average='binary')\n",
    "\n",
    "hybrid_train_f1 = f1_score(y_train_binary, hybrid_train_pred, average='binary')\n",
    "hybrid_val_f1 = f1_score(y_val_binary, hybrid_val_pred, average='binary')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'üìä HYBRID MODEL PERFORMANCE')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'\\nTraining Metrics:')\n",
    "print(f'  Accuracy:  {hybrid_train_acc:.4f}')\n",
    "print(f'  Precision: {hybrid_train_precision:.4f}')\n",
    "print(f'  Recall:    {hybrid_train_recall:.4f}')\n",
    "print(f'  F1-Score:  {hybrid_train_f1:.4f}')\n",
    "\n",
    "print(f'\\nValidation Metrics:')\n",
    "print(f'  Accuracy:  {hybrid_val_acc:.4f}')\n",
    "print(f'  Precision: {hybrid_val_precision:.4f}')\n",
    "print(f'  Recall:    {hybrid_val_recall:.4f}')\n",
    "print(f'  F1-Score:  {hybrid_val_f1:.4f}')\n",
    "\n",
    "# Meta-learner weights (how much LSTM vs RF)\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Meta-Learner Weights:')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'  LSTM weight: {meta_learner.coef_[0][0]:+.4f}')\n",
    "print(f'  RF weight:   {meta_learner.coef_[0][1]:+.4f}')\n",
    "print(f'  Intercept:   {meta_learner.intercept_[0]:+.4f}')\n",
    "\n",
    "lstm_importance = abs(meta_learner.coef_[0][0])\n",
    "rf_importance = abs(meta_learner.coef_[0][1])\n",
    "total_importance = lstm_importance + rf_importance\n",
    "\n",
    "print(f'\\nRelative Importance:')\n",
    "print(f'  LSTM: {(lstm_importance/total_importance)*100:.1f}%')\n",
    "print(f'  RF:   {(rf_importance/total_importance)*100:.1f}%')\n",
    "\n",
    "# Confusion Matrix & report\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Validation Confusion Matrix:')\n",
    "print(f'{\"=\"*60}')\n",
    "cm = confusion_matrix(y_val_binary, hybrid_val_pred)\n",
    "print(cm)\n",
    "\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(\n",
    "    y_val_binary,\n",
    "    hybrid_val_pred,\n",
    "    target_names=['Not At Risk', 'At Risk']\n",
    "))\n",
    "\n",
    "# ======================================================\n",
    "# 3. SAVE MODELS\n",
    "# ======================================================\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'üíæ SAVING MODELS')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print('\\nSaving individual models...')\n",
    "\n",
    "# LSTM model\n",
    "lstm_model_path = f'lstm_model_{timestamp}.h5'\n",
    "lstm_model.save(lstm_model_path)\n",
    "print(f'‚úì LSTM model saved: {lstm_model_path}')\n",
    "\n",
    "# Random Forest model\n",
    "rf_model_path = f'rf_model_{timestamp}.pkl'\n",
    "joblib.dump(rf_model, rf_model_path)\n",
    "print(f'‚úì Random Forest model saved: {rf_model_path}')\n",
    "\n",
    "# Meta-learner\n",
    "meta_learner_path = f'meta_learner_{timestamp}.pkl'\n",
    "joblib.dump(meta_learner, meta_learner_path)\n",
    "print(f'‚úì Meta-learner saved: {meta_learner_path}')\n",
    "\n",
    "# ======================================================\n",
    "# 4. SAVE PREPROCESSING OBJECTS (FIXED NameErrors)\n",
    "# ======================================================\n",
    "\n",
    "print('\\nSaving preprocessing objects...')\n",
    "\n",
    "# Ensure label_encoder exists (binary 0/1)\n",
    "if 'label_encoder' not in globals():\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit([0, 1])\n",
    "\n",
    "# Ensure all referenced config variables exist (do NOT overwrite if they already exist)\n",
    "if 'scaler' not in globals():\n",
    "    scaler = None\n",
    "\n",
    "if 'label_encoders' not in globals():\n",
    "    label_encoders = {}\n",
    "\n",
    "if 'static_feature_cols' not in globals():\n",
    "    static_feature_cols = []\n",
    "\n",
    "if 'actual_numerical' not in globals():\n",
    "    actual_numerical = []\n",
    "\n",
    "if 'actual_categorical' not in globals():\n",
    "    actual_categorical = []\n",
    "\n",
    "if 'actual_binary' not in globals():\n",
    "    actual_binary = []\n",
    "\n",
    "if 'sequence_length' not in globals():\n",
    "    sequence_length = None\n",
    "\n",
    "if 'lstm_features' not in globals():\n",
    "    lstm_features = None\n",
    "\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'label_encoder': label_encoder,  # For target variable\n",
    "    'static_feature_cols': static_feature_cols,\n",
    "    'actual_numerical': actual_numerical,\n",
    "    'actual_categorical': actual_categorical,\n",
    "    'actual_binary': actual_binary,\n",
    "    'sequence_length': sequence_length,\n",
    "    'lstm_features': lstm_features,\n",
    "    'threshold': threshold\n",
    "}\n",
    "\n",
    "preprocessing_path = f'preprocessing_objects_{timestamp}.pkl'\n",
    "joblib.dump(preprocessing_objects, preprocessing_path)\n",
    "print(f'‚úì Preprocessing objects saved: {preprocessing_path}')\n",
    "\n",
    "# ======================================================\n",
    "# 5. SAVE COMPLETE HYBRID PACKAGE\n",
    "# ======================================================\n",
    "\n",
    "print('\\nSaving complete hybrid model package...')\n",
    "\n",
    "hybrid_model_package = {\n",
    "    'meta_learner': meta_learner,\n",
    "    'preprocessing_objects': preprocessing_objects,\n",
    "    'lstm_model_path': lstm_model_path,\n",
    "    'rf_model_path': rf_model_path,\n",
    "    'model_performance': {\n",
    "        'train_accuracy': hybrid_train_acc,\n",
    "        'val_accuracy': hybrid_val_acc,\n",
    "        'train_f1': hybrid_train_f1,\n",
    "        'val_f1': hybrid_val_f1,\n",
    "        'train_precision': hybrid_train_precision,\n",
    "        'val_precision': hybrid_val_precision,\n",
    "        'train_recall': hybrid_train_recall,\n",
    "        'val_recall': hybrid_val_recall\n",
    "    },\n",
    "    'meta_learner_weights': {\n",
    "        'lstm_weight': float(meta_learner.coef_[0][0]),\n",
    "        'rf_weight': float(meta_learner.coef_[0][1]),\n",
    "        'intercept': float(meta_learner.intercept_[0])\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "hybrid_package_path = f'hybrid_model_complete_{timestamp}.pkl'\n",
    "joblib.dump(hybrid_model_package, hybrid_package_path)\n",
    "print(f'‚úì Complete hybrid model package saved: {hybrid_package_path}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'‚úÖ ALL MODELS SAVED SUCCESSFULLY')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# ======================================================\n",
    "# 6. TOP-LEVEL PREDICTION FUNCTION (PICKLABLE)\n",
    "# ======================================================\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def predict_student_risk(\n",
    "    lstm_model_path,\n",
    "    rf_model_path,\n",
    "    meta_learner_path,\n",
    "    preprocessing_path,\n",
    "    student_data_static,\n",
    "    student_data_sequence\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict student dropout risk using the hybrid model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lstm_model_path : str\n",
    "        Path to saved LSTM model\n",
    "    rf_model_path : str\n",
    "        Path to saved Random Forest model\n",
    "    meta_learner_path : str\n",
    "        Path to saved meta-learner\n",
    "    preprocessing_path : str\n",
    "        Path to preprocessing objects\n",
    "    student_data_static : DataFrame\n",
    "        Static features for the student (1 row, already preprocessed)\n",
    "    student_data_sequence : array\n",
    "        Sequential data for the student (shape: (1, sequence_length, lstm_features), already preprocessed)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Prediction results including probability and risk level\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models\n",
    "    lstm_model = keras.models.load_model(lstm_model_path)\n",
    "    rf_model = joblib.load(rf_model_path)\n",
    "    meta_learner = joblib.load(meta_learner_path)\n",
    "    preprocessing = joblib.load(preprocessing_path)\n",
    "\n",
    "    # TODO: Apply same preprocessing as training to raw inputs if needed.\n",
    "    # Here we assume student_data_static and student_data_sequence are already in model-ready form.\n",
    "\n",
    "    # Base model predictions\n",
    "    lstm_pred = lstm_model.predict(student_data_sequence, verbose=0)[0][0]\n",
    "    rf_pred = rf_model.predict_proba(student_data_static)[0][1]\n",
    "\n",
    "    # Meta-features\n",
    "    meta_features = np.array([[lstm_pred, rf_pred]])\n",
    "\n",
    "    # Final prediction\n",
    "    risk_probability = meta_learner.predict_proba(meta_features)[0][1]\n",
    "    risk_prediction = int(risk_probability > preprocessing['threshold'])\n",
    "\n",
    "    # Risk level\n",
    "    if risk_probability < 0.3:\n",
    "        risk_level = 'Low Risk'\n",
    "    elif risk_probability < 0.7:\n",
    "        risk_level = 'Medium Risk'\n",
    "    else:\n",
    "        risk_level = 'High Risk'\n",
    "\n",
    "    return {\n",
    "        'risk_probability': float(risk_probability),\n",
    "        'at_risk': bool(risk_prediction),\n",
    "        'risk_level': risk_level,\n",
    "        'lstm_contribution': float(lstm_pred),\n",
    "        'rf_contribution': float(rf_pred)\n",
    "    }\n",
    "\n",
    "# Save the prediction function (now top-level ‚Üí picklable)\n",
    "prediction_function_path = f'prediction_function_{timestamp}.pkl'\n",
    "joblib.dump(predict_student_risk, prediction_function_path)\n",
    "print(f'\\n‚úì Prediction function saved: {prediction_function_path}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'üìù USAGE INSTRUCTIONS')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'''\n",
    "# Load the complete package\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "hybrid_package = joblib.load('{hybrid_package_path}')\n",
    "lstm_model = keras.models.load_model('{lstm_model_path}')\n",
    "rf_model = joblib.load('{rf_model_path}')\n",
    "predict_student_risk = joblib.load('{prediction_function_path}')\n",
    "\n",
    "# Example (assuming you have preprocessed student_data_static and student_data_sequence):\n",
    "# result = predict_student_risk(\n",
    "#     lstm_model_path='{lstm_model_path}',\n",
    "#     rf_model_path='{rf_model_path}',\n",
    "#     meta_learner_path='{meta_learner_path}',\n",
    "#     preprocessing_path='{preprocessing_path}',\n",
    "#     student_data_static=student_data_static,\n",
    "#     student_data_sequence=student_data_sequence\n",
    "# )\n",
    "# print(result)\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Model Comparison & Ablation Study\n",
    "\n",
    "Before building the hybrid model, let's compare individual model performances to justify the hybrid approach.\n",
    "\n",
    "### **Individual Model Strengths:**\n",
    "\n",
    "| Model | Strengths | Limitations |\n",
    "|-------|-----------|-------------|\n",
    "| **LSTM** | ‚úì Temporal pattern recognition<br>‚úì Captures engagement evolution<br>‚úì Detects early warning signals | ‚úó Ignores static features<br>‚úó Requires sequential data<br>‚úó Less interpretable |\n",
    "| **Random Forest** | ‚úì Handles mixed data types<br>‚úì Feature importance<br>‚úì Robust to outliers | ‚úó Ignores temporal dynamics<br>‚úó No sequential learning<br>‚úó Snapshot-based only |\n",
    "| **Hybrid (LSTM+RF)** | ‚úì Temporal + static features<br>‚úì Complementary strengths<br>‚úì Better generalization | ‚úó More complex<br>‚úó Higher computational cost |\n",
    "\n",
    "### **Expected Improvement:**\n",
    "Hybrid model should outperform individual models by 5-15% on validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('üìä COMPREHENSIVE MODEL COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "# Collect all metrics from previous models\n",
    "comparison_data = {\n",
    "    'Model': ['LSTM (Temporal Only)', 'Random Forest (Static Only)', 'Hybrid (LSTM + RF)'],\n",
    "    'Train_Accuracy': [\n",
    "        lstm_train_acc,\n",
    "        rf_train_acc,\n",
    "        0.0  # Will be filled after hybrid training\n",
    "    ],\n",
    "    'Val_Accuracy': [\n",
    "        lstm_val_acc,\n",
    "        rf_val_acc,\n",
    "        0.0  # Will be filled after hybrid training\n",
    "    ],\n",
    "    'Train_Precision': [\n",
    "        precision_score(y_train > 0.5, lstm_train_pred > 0.5, zero_division=0),\n",
    "        rf_train_precision,\n",
    "        0.0\n",
    "    ],\n",
    "    'Val_Precision': [\n",
    "        precision_score(y_val > 0.5, lstm_val_pred > 0.5, zero_division=0),\n",
    "        rf_val_precision,\n",
    "        0.0\n",
    "    ],\n",
    "    'Train_Recall': [\n",
    "        recall_score(y_train > 0.5, lstm_train_pred > 0.5, zero_division=0),\n",
    "        rf_train_recall,\n",
    "        0.0\n",
    "    ],\n",
    "    'Val_Recall': [\n",
    "        recall_score(y_val > 0.5, lstm_val_pred > 0.5, zero_division=0),\n",
    "        rf_val_recall,\n",
    "        0.0\n",
    "    ],\n",
    "    'Train_F1': [\n",
    "        f1_score(y_train > 0.5, lstm_train_pred > 0.5, zero_division=0),\n",
    "        rf_train_f1,\n",
    "        0.0\n",
    "    ],\n",
    "    'Val_F1': [\n",
    "        f1_score(y_val > 0.5, lstm_val_pred > 0.5, zero_division=0),\n",
    "        rf_val_f1,\n",
    "        0.0\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print('\\nüìà Current Model Performance (Before Hybrid):')\n",
    "print('‚îÄ' * 80)\n",
    "print(comparison_df[['Model', 'Val_Accuracy', 'Val_Precision', 'Val_Recall', 'Val_F1']].to_string(index=False))\n",
    "\n",
    "print('\\nüí° Observation:')\n",
    "print('  - LSTM captures temporal patterns but lacks demographic/cultural context')\n",
    "print('  - Random Forest captures static patterns but misses engagement dynamics')\n",
    "print('  - Hybrid model will combine both strengths for superior prediction')\n",
    "\n",
    "print('\\n‚è≠Ô∏è Proceeding to build hybrid meta-learner...')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'Random Forest', 'Hybrid (Meta-Learner)'],\n",
    "    'Training Accuracy': [lstm_train_acc, rf_train_acc, hybrid_train_acc],\n",
    "    'Validation Accuracy': [lstm_val_acc, rf_val_acc, hybrid_val_acc],\n",
    "    'Difference (Val-Train)': [\n",
    "        lstm_val_acc - lstm_train_acc,\n",
    "        rf_val_acc - rf_train_acc,\n",
    "        hybrid_val_acc - hybrid_train_acc\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('='*60)\n",
    "print('MODEL PERFORMANCE COMPARISON')\n",
    "print('='*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('='*60)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Training Accuracy'], width, label='Training', color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Validation Accuracy'], width, label='Validation', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Predictions for All Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for validation set\n",
    "df_val['predicted_success_proba'] = hybrid_val_pred_proba\n",
    "\n",
    "# Map to risk levels\n",
    "def map_to_risk_level(prob):\n",
    "    if prob >= 0.67:\n",
    "        return 'Low Risk'\n",
    "    elif prob >= 0.33:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'High Risk'\n",
    "\n",
    "df_val['predicted_risk_level'] = df_val['predicted_success_proba'].apply(map_to_risk_level)\n",
    "\n",
    "# Create success label from risk (Low/Medium = success, High = not)\n",
    "df_val['success_label_from_risk'] = df_val['predicted_risk_level'].apply(\n",
    "    lambda x: 'Success' if x in ['Low Risk', 'Medium Risk'] else 'At Risk'\n",
    ")\n",
    "\n",
    "print('Prediction Summary:')\n",
    "print(f'Total students predicted: {len(df_val)}')\n",
    "print(f'Average success probability: {df_val[\"predicted_success_proba\"].mean():.3f}')\n",
    "print('\\nRisk Level Distribution:')\n",
    "print(df_val['predicted_risk_level'].value_counts())\n",
    "print('\\nSuccess Label Distribution:')\n",
    "print(df_val['success_label_from_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Global Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Global Risk Distribution\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Predicted Risk Level Distribution', 'Success vs At-Risk Distribution'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    ")\n",
    "\n",
    "# Risk levels bar chart\n",
    "risk_counts = df_val['predicted_risk_level'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=risk_counts.index, y=risk_counts.values,\n",
    "           marker_color=['green', 'orange', 'red']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Success pie chart\n",
    "success_counts = df_val['success_label_from_risk'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=success_counts.index, values=success_counts.values,\n",
    "           marker_colors=['#2ecc71', '#e74c3c']),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, showlegend=False,\n",
    "                  title_text='Global Student Risk and Success Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Average Weekly Engagement Pattern by Risk Level\n",
    "# Calculate average engagement patterns for each risk level\n",
    "risk_engagement = {}\n",
    "\n",
    "for risk_level in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "    # Get indices of students in this risk level\n",
    "    risk_indices = df_val[df_val['predicted_risk_level'] == risk_level].index\n",
    "    val_risk_indices = [i for i, idx in enumerate(val_idx) if idx in risk_indices]\n",
    "    \n",
    "    if len(val_risk_indices) > 0:\n",
    "        # Get temporal sequences for these students\n",
    "        risk_sequences = X_temporal_val[val_risk_indices]\n",
    "        # Average across students (column 0 is weekly_engagement)\n",
    "        avg_engagement = np.mean(risk_sequences[:, :, 0], axis=0)\n",
    "        risk_engagement[risk_level] = avg_engagement\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "weeks = np.arange(1, 33)\n",
    "colors = {'Low Risk': 'green', 'Medium Risk': 'orange', 'High Risk': 'red'}\n",
    "\n",
    "for risk_level, engagement in risk_engagement.items():\n",
    "    plt.plot(weeks, engagement, label=risk_level, \n",
    "             color=colors[risk_level], linewidth=2, marker='o', markersize=3)\n",
    "\n",
    "plt.xlabel('Week', fontsize=12)\n",
    "plt.ylabel('Average Normalized Engagement', fontsize=12)\n",
    "plt.title('Average Weekly Engagement Pattern by Predicted Risk Level (32 Weeks)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Latvia-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Latvia-hosted students\n",
    "df_latvia = df_val[df_val['country_host'] == 'Latvia'].copy()\n",
    "\n",
    "print(f'Latvia-Specific Analysis')\n",
    "print(f'Total international students in Latvia: {len(df_latvia)}')\n",
    "print(f'\\nLatvian institutions represented:')\n",
    "print(df_latvia['institution'].value_counts())\n",
    "\n",
    "# Success vs At-Risk for Latvia\n",
    "if len(df_latvia) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Pie chart for Success vs At-Risk\n",
    "    latvia_success = df_latvia['success_label_from_risk'].value_counts()\n",
    "    axes[0].pie(latvia_success.values, labels=latvia_success.index, \n",
    "                autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
    "    axes[0].set_title('Predicted Success vs At-Risk\\n(Latvia-hosted International Students)', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Bar chart by institution\n",
    "    inst_success = df_latvia.groupby('institution')['predicted_success_proba'].mean().sort_values(ascending=False)\n",
    "    axes[1].barh(range(len(inst_success)), inst_success.values, color='steelblue')\n",
    "    axes[1].set_yticks(range(len(inst_success)))\n",
    "    axes[1].set_yticklabels(inst_success.index)\n",
    "    axes[1].set_xlabel('Average Success Probability')\n",
    "    axes[1].set_title('Success Probability by Latvian Institution', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No Latvia-hosted students in validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latvia Student Details Table\n",
    "if len(df_latvia) > 0:\n",
    "    # Prepare table data\n",
    "    latvia_table = df_latvia[[\n",
    "        'country_home', 'institution', 'subject_field',\n",
    "        'predicted_success_proba', 'predicted_risk_level',\n",
    "        'success_label_from_risk', 'mean_weekly_engagement',\n",
    "        'attendance_rate'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by success probability\n",
    "    latvia_table = latvia_table.sort_values('predicted_success_proba', ascending=False)\n",
    "    \n",
    "    # Round numerical columns\n",
    "    latvia_table['predicted_success_proba'] = latvia_table['predicted_success_proba'].round(3)\n",
    "    latvia_table['mean_weekly_engagement'] = latvia_table['mean_weekly_engagement'].round(2)\n",
    "    latvia_table['attendance_rate'] = latvia_table['attendance_rate'].round(2)\n",
    "    \n",
    "    print('\\nLatvia-Hosted International Students Details (Top 20):')\n",
    "    print('='*100)\n",
    "    print(latvia_table.head(20).to_string(index=False))\n",
    "    print('='*100)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f'\\nSummary Statistics for Latvia:')\n",
    "    print(f'Average success probability: {df_latvia[\"predicted_success_proba\"].mean():.3f}')\n",
    "    print(f'Students at high risk: {(df_latvia[\"predicted_risk_level\"] == \"High Risk\").sum()} ({(df_latvia[\"predicted_risk_level\"] == \"High Risk\").mean()*100:.1f}%)')\n",
    "    print(f'Top countries by count: {df_latvia[\"country_home\"].value_counts().head(5).to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Success & Risk by Subject Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate success rates by subject field\n",
    "subject_analysis = df_val.groupby('subject_field').agg({\n",
    "    'predicted_success_proba': 'mean',\n",
    "    'success_label_from_risk': lambda x: (x == 'Success').mean(),\n",
    "    'student_id': 'count'\n",
    "}).round(3)\n",
    "\n",
    "subject_analysis.columns = ['Avg_Success_Prob', 'Success_Rate', 'Student_Count']\n",
    "subject_analysis = subject_analysis.sort_values('Success_Rate', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "subjects = subject_analysis.index\n",
    "success_rates = subject_analysis['Success_Rate'].values\n",
    "\n",
    "bars = ax.barh(range(len(subjects)), success_rates, color='teal')\n",
    "ax.set_yticks(range(len(subjects)))\n",
    "ax.set_yticklabels(subjects)\n",
    "ax.set_xlabel('Success Rate (Proportion of Students Predicted as Successful)', fontsize=12)\n",
    "ax.set_title('Success Rate by Subject Field', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, subject_analysis['Student_Count'])):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.2f} (n={count})', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Subject Field Analysis:')\n",
    "print(subject_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Average Success Probability by Country of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average success probability by country\n",
    "country_analysis = df_val.groupby('country_home').agg({\n",
    "    'predicted_success_proba': 'mean',\n",
    "    'student_id': 'count'\n",
    "}).round(3)\n",
    "\n",
    "country_analysis.columns = ['Avg_Success_Prob', 'Student_Count']\n",
    "\n",
    "# Filter countries with at least 2 students and get top 15\n",
    "country_analysis = country_analysis[country_analysis['Student_Count'] >= 2]\n",
    "country_analysis = country_analysis.sort_values('Avg_Success_Prob', ascending=False).head(15)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "countries = country_analysis.index\n",
    "success_probs = country_analysis['Avg_Success_Prob'].values\n",
    "counts = country_analysis['Student_Count'].values\n",
    "\n",
    "# Color gradient based on success probability\n",
    "colors = plt.cm.RdYlGn(success_probs)  # Red to Yellow to Green colormap\n",
    "\n",
    "bars = plt.bar(range(len(countries)), success_probs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Country of Origin', fontsize=12)\n",
    "plt.ylabel('Average Predicted Success Probability', fontsize=12)\n",
    "plt.title('Average Predicted Success Probability by Country of Origin (Top 15)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(countries)), countries, rotation=45, ha='right')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, prob, count in zip(bars, success_probs, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{prob:.2f}\\n(n={count})', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 15 Countries by Average Success Probability:')\n",
    "print(country_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('HYBRID PREDICTION FRAMEWORK - SUMMARY REPORT')\n",
    "print('='*80)\n",
    "\n",
    "print(f'''\n",
    "üìä DATASET OVERVIEW:\n",
    "   ‚Ä¢ Total students analyzed: {len(df_static)}\n",
    "   ‚Ä¢ Training set: {len(train_idx)} students\n",
    "   ‚Ä¢ Validation set: {len(val_idx)} students\n",
    "   ‚Ä¢ Institutions: {df_static[\"institution\"].nunique()}\n",
    "   ‚Ä¢ Countries: {df_static[\"country_home\"].nunique()}\n",
    "   ‚Ä¢ Subject fields: {df_static[\"subject_field\"].nunique()}\n",
    "\n",
    "üéØ MODEL PERFORMANCE:\n",
    "   LSTM Model:\n",
    "   ‚Ä¢ Training Accuracy: {lstm_train_acc:.4f}\n",
    "   ‚Ä¢ Validation Accuracy: {lstm_val_acc:.4f}\n",
    "   \n",
    "   Random Forest Model:\n",
    "   ‚Ä¢ Training Accuracy: {rf_train_acc:.4f}\n",
    "   ‚Ä¢ Validation Accuracy: {rf_val_acc:.4f}\n",
    "   \n",
    "   Hybrid Meta-Learner:\n",
    "   ‚Ä¢ Training Accuracy: {hybrid_train_acc:.4f}\n",
    "   ‚Ä¢ Validation Accuracy: {hybrid_val_acc:.4f}\n",
    "   ‚Ä¢ Meta-weights: LSTM={meta_learner.coef_[0][0]:.3f}, RF={meta_learner.coef_[0][1]:.3f}\n",
    "\n",
    "üìà PREDICTION INSIGHTS:\n",
    "   Global Analysis:\n",
    "   ‚Ä¢ Students predicted as successful: {(df_val[\"success_label_from_risk\"] == \"Success\").sum()} ({(df_val[\"success_label_from_risk\"] == \"Success\").mean()*100:.1f}%)\n",
    "   ‚Ä¢ Students at high risk: {(df_val[\"predicted_risk_level\"] == \"High Risk\").sum()} ({(df_val[\"predicted_risk_level\"] == \"High Risk\").mean()*100:.1f}%)\n",
    "   ‚Ä¢ Average success probability: {df_val[\"predicted_success_proba\"].mean():.3f}\n",
    "''')\n",
    "\n",
    "if len(df_latvia) > 0:\n",
    "    print(f'''\n",
    "üá±üáª LATVIA-SPECIFIC INSIGHTS:\n",
    "   ‚Ä¢ International students in Latvia: {len(df_latvia)}\n",
    "   ‚Ä¢ Average success probability: {df_latvia[\"predicted_success_proba\"].mean():.3f}\n",
    "   ‚Ä¢ High-risk students: {(df_latvia[\"predicted_risk_level\"] == \"High Risk\").sum()} ({(df_latvia[\"predicted_risk_level\"] == \"High Risk\").mean()*100:.1f}%)\n",
    "   ‚Ä¢ Top source countries: {df_latvia[\"country_home\"].value_counts().head(3).to_dict()}\n",
    "''')\n",
    "\n",
    "print(f'''\n",
    "üîç KEY FINDINGS:\n",
    "   1. The Hybrid model achieves the best balance between training and validation accuracy\n",
    "   2. Most important features: {list(feature_importance.head(5)[\"feature\"].values)}\n",
    "   3. Subject fields with highest success rates: {list(subject_analysis.head(3).index)}\n",
    "   4. Countries with highest success probability: {list(country_analysis.head(3).index)}\n",
    "\n",
    "‚úÖ FRAMEWORK SUCCESSFULLY IMPLEMENTED\n",
    "   ‚Ä¢ LSTM captures temporal engagement patterns\n",
    "   ‚Ä¢ Random Forest leverages static features\n",
    "   ‚Ä¢ Meta-learner optimally combines both approaches\n",
    "   ‚Ä¢ Predictions are explainable and actionable\n",
    "''')\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 17. COMPREHENSIVE RISK CATEGORY ANALYSIS & STUDENT SUCCESS PREDICTION\n",
    "\n",
    "Advanced Visualizations for Risk Identification and Cluster Differentiation\n",
    "\"\"\"\n",
    "\n",
    "# Cell Code to Add:\n",
    "cell_code = \"\"\"\n",
    "## 17. COMPREHENSIVE RISK CATEGORY ANALYSIS & STUDENT SUCCESS PREDICTION\n",
    "\n",
    "This section provides advanced visualizations to identify students by risk category,\n",
    "analyze cluster differences, and predict pass/fail outcomes.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\" \" * 25 + \"üìä RISK CATEGORY & SUCCESS ANALYSIS üìä\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. RISK CATEGORY DISTRIBUTION WITH SUCCESS LABELS\n",
    "# ============================================================================\n",
    "print(\"\\nüéØ 1. OVERALL RISK DISTRIBUTION & SUCCESS RATES\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Create comprehensive risk analysis dataframe\n",
    "risk_success_summary = df_val.groupby(['predicted_risk_level', 'success_label_from_risk']).agg({\n",
    "    'student_id': 'count',\n",
    "    'predicted_success_proba': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "risk_success_summary.columns = ['Count', 'Avg_Success_Prob', 'Std_Success_Prob']\n",
    "print(\"\\nRisk Category √ó Success Label Cross-Tabulation:\")\n",
    "print(risk_success_summary)\n",
    "\n",
    "# Visualization 1: Stacked Bar Chart - Risk Distribution with Success Labels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: Stacked bar for risk levels\n",
    "risk_counts = df_val.groupby(['predicted_risk_level', 'success_label_from_risk']).size().unstack(fill_value=0)\n",
    "risk_counts_ordered = risk_counts.reindex(['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "colors_success = ['#2ecc71', '#e74c3c']  # Green for Success, Red for At Risk\n",
    "risk_counts_ordered.plot(kind='bar', stacked=True, ax=axes[0], color=colors_success, \n",
    "                         edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_title('Risk Category Distribution with Success Labels', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Risk Level', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Students', fontsize=12)\n",
    "axes[0].legend(title='Predicted Outcome', fontsize=10)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, label_type='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: Success rate by risk level\n",
    "success_rates = df_val.groupby('predicted_risk_level').apply(\n",
    "    lambda x: (x['success_label_from_risk'] == 'Success').mean() * 100\n",
    ").reindex(['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "bars = axes[1].bar(success_rates.index, success_rates.values, \n",
    "                   color=['#27ae60', '#f39c12', '#c0392b'], edgecolor='black', linewidth=1.2)\n",
    "axes[1].set_title('Success Rate by Risk Category', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Risk Level', fontsize=12)\n",
    "axes[1].set_ylabel('Success Rate (%)', fontsize=12)\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CLUSTER-BASED RISK ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüî¨ 2. CLUSTERING ANALYSIS BY RISK CATEGORY\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare features for clustering (numerical features only)\n",
    "cluster_features = ['predicted_success_proba', 'mean_weekly_engagement', 'attendance_rate', \n",
    "                   'avg_assignment_score', 'avg_exam_score', 'gpa_sem1', 'gpa_sem2',\n",
    "                   'low_engagement_weeks', 'failed_courses_sem1', 'failed_courses_sem2']\n",
    "\n",
    "X_cluster = df_val[cluster_features].copy()\n",
    "X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
    "\n",
    "# Perform K-Means clustering (4 clusters)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df_val['cluster_label'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Map clusters to intuitive names based on success probability\n",
    "cluster_means = df_val.groupby('cluster_label')['predicted_success_proba'].mean().sort_values(ascending=False)\n",
    "cluster_mapping = {\n",
    "    cluster_means.index[0]: 'Elite Performers',\n",
    "    cluster_means.index[1]: 'Strong Achievers',\n",
    "    cluster_means.index[2]: 'Moderate Performers',\n",
    "    cluster_means.index[3]: 'At-Risk Students'\n",
    "}\n",
    "df_val['cluster_name'] = df_val['cluster_label'].map(cluster_mapping)\n",
    "\n",
    "print(\"\\nCluster Statistics:\")\n",
    "cluster_stats = df_val.groupby('cluster_name').agg({\n",
    "    'student_id': 'count',\n",
    "    'predicted_success_proba': ['mean', 'std'],\n",
    "    'mean_weekly_engagement': 'mean',\n",
    "    'gpa_sem1': 'mean',\n",
    "    'attendance_rate': 'mean'\n",
    "}).round(3)\n",
    "cluster_stats.columns = ['Count', 'Avg_Success_Prob', 'Std', 'Avg_Engagement', 'Avg_GPA', 'Avg_Attendance']\n",
    "print(cluster_stats)\n",
    "\n",
    "# Cross-tabulation: Cluster vs Risk Level\n",
    "print(\"\\n\\nCluster √ó Risk Level Cross-Tabulation:\")\n",
    "cluster_risk_crosstab = pd.crosstab(df_val['cluster_name'], df_val['predicted_risk_level'])\n",
    "cluster_risk_crosstab = cluster_risk_crosstab[['Low Risk', 'Medium Risk', 'High Risk']]\n",
    "print(cluster_risk_crosstab)\n",
    "\n",
    "# Visualization 2: Cluster Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: PCA visualization of clusters\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "\n",
    "cluster_colors = {'Elite Performers': '#2ecc71', 'Strong Achievers': '#3498db', \n",
    "                  'Moderate Performers': '#f39c12', 'At-Risk Students': '#e74c3c'}\n",
    "\n",
    "for cluster_name in df_val['cluster_name'].unique():\n",
    "    mask = df_val['cluster_name'] == cluster_name\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   label=cluster_name, alpha=0.6, s=80, \n",
    "                   color=cluster_colors[cluster_name], edgecolors='black', linewidth=0.5)\n",
    "\n",
    "axes[0].set_title('Student Clusters in PCA Space', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)\n",
    "axes[0].legend(title='Cluster', fontsize=9, loc='best')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: Heatmap of cluster characteristics\n",
    "cluster_profiles = df_val.groupby('cluster_name')[\n",
    "    ['predicted_success_proba', 'mean_weekly_engagement', 'attendance_rate', \n",
    "     'avg_assignment_score', 'gpa_sem1']\n",
    "].mean()\n",
    "cluster_profiles = cluster_profiles.reindex(['Elite Performers', 'Strong Achievers', \n",
    "                                             'Moderate Performers', 'At-Risk Students'])\n",
    "\n",
    "# Normalize for heatmap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "cluster_profiles_norm = pd.DataFrame(\n",
    "    MinMaxScaler().fit_transform(cluster_profiles),\n",
    "    index=cluster_profiles.index,\n",
    "    columns=['Success Prob', 'Engagement', 'Attendance', 'Assignment Score', 'GPA']\n",
    ")\n",
    "\n",
    "sns.heatmap(cluster_profiles_norm.T, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Normalized Score'}, ax=axes[1], \n",
    "            linewidths=0.5, linecolor='black')\n",
    "axes[1].set_title('Cluster Performance Profiles (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster', fontsize=12)\n",
    "axes[1].set_ylabel('Performance Metrics', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. PASS/FAIL PREDICTION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\\n‚úÖ‚ùå 3. PASS/FAIL PREDICTION BREAKDOWN\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Create pass/fail labels based on success probability threshold\n",
    "df_val['pass_fail_prediction'] = df_val['predicted_success_proba'].apply(\n",
    "    lambda x: 'PASS' if x >= 0.5 else 'FAIL'\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "pass_fail_summary = df_val.groupby('pass_fail_prediction').agg({\n",
    "    'student_id': 'count',\n",
    "    'predicted_success_proba': ['mean', 'min', 'max'],\n",
    "    'gpa_sem1': 'mean',\n",
    "    'attendance_rate': 'mean',\n",
    "    'mean_weekly_engagement': 'mean'\n",
    "}).round(3)\n",
    "pass_fail_summary.columns = ['Count', 'Avg_Prob', 'Min_Prob', 'Max_Prob', 'Avg_GPA', 'Avg_Attendance', 'Avg_Engagement']\n",
    "print(\"\\nPass/Fail Prediction Summary:\")\n",
    "print(pass_fail_summary)\n",
    "\n",
    "# Risk level breakdown by pass/fail\n",
    "print(\"\\n\\nPass/Fail √ó Risk Level Distribution:\")\n",
    "pass_fail_risk = pd.crosstab(df_val['pass_fail_prediction'], df_val['predicted_risk_level'], \n",
    "                              margins=True, margins_name='Total')\n",
    "print(pass_fail_risk)\n",
    "\n",
    "# Visualization 3: Pass/Fail Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Top-left: Pass/Fail pie chart\n",
    "pass_fail_counts = df_val['pass_fail_prediction'].value_counts()\n",
    "colors_pie = ['#2ecc71', '#e74c3c']\n",
    "explode = (0.05, 0.05)\n",
    "\n",
    "axes[0, 0].pie(pass_fail_counts.values, labels=pass_fail_counts.index, autopct='%1.1f%%',\n",
    "               colors=colors_pie, explode=explode, shadow=True, startangle=90,\n",
    "               textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[0, 0].set_title('Overall Pass/Fail Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Top-right: Pass/Fail by risk level (grouped bar)\n",
    "pass_fail_risk_plot = pd.crosstab(df_val['predicted_risk_level'], df_val['pass_fail_prediction'])\n",
    "pass_fail_risk_plot = pass_fail_risk_plot.reindex(['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "pass_fail_risk_plot.plot(kind='bar', ax=axes[0, 1], color=['#e74c3c', '#2ecc71'], \n",
    "                         edgecolor='black', linewidth=1.2)\n",
    "axes[0, 1].set_title('Pass/Fail Predictions by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Level', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Number of Students', fontsize=12)\n",
    "axes[0, 1].legend(title='Prediction', fontsize=10)\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Bottom-left: Success probability distribution by pass/fail\n",
    "df_pass = df_val[df_val['pass_fail_prediction'] == 'PASS']['predicted_success_proba']\n",
    "df_fail = df_val[df_val['pass_fail_prediction'] == 'FAIL']['predicted_success_proba']\n",
    "\n",
    "axes[1, 0].hist(df_pass, bins=30, alpha=0.7, color='#2ecc71', label='PASS', edgecolor='black')\n",
    "axes[1, 0].hist(df_fail, bins=30, alpha=0.7, color='#e74c3c', label='FAIL', edgecolor='black')\n",
    "axes[1, 0].axvline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1, 0].set_title('Success Probability Distribution by Pass/Fail', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted Success Probability', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Number of Students', fontsize=12)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Bottom-right: Box plot - GPA by Pass/Fail and Risk\n",
    "df_val_plot = df_val.copy()\n",
    "df_val_plot['Risk_PassFail'] = df_val_plot['predicted_risk_level'] + ' - ' + df_val_plot['pass_fail_prediction']\n",
    "\n",
    "risk_order = ['Low Risk - PASS', 'Low Risk - FAIL', 'Medium Risk - PASS', \n",
    "              'Medium Risk - FAIL', 'High Risk - PASS', 'High Risk - FAIL']\n",
    "df_val_plot['Risk_PassFail'] = pd.Categorical(df_val_plot['Risk_PassFail'], \n",
    "                                               categories=risk_order, ordered=True)\n",
    "\n",
    "sns.boxplot(data=df_val_plot, x='Risk_PassFail', y='gpa_sem1', ax=axes[1, 1],\n",
    "            palette=['#27ae60', '#c0392b', '#f39c12', '#e67e22', '#e74c3c', '#8b0000'])\n",
    "axes[1, 1].set_title('GPA Distribution by Risk Level and Pass/Fail Prediction', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Risk Level - Pass/Fail', fontsize=11)\n",
    "axes[1, 1].set_ylabel('GPA (Semester 1)', fontsize=12)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DETAILED STUDENT LISTS BY RISK CATEGORY\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìã 4. DETAILED STUDENT LISTS BY RISK CATEGORY\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Function to display student details\n",
    "def display_student_list(risk_level, max_students=20):\n",
    "    students = df_val[df_val['predicted_risk_level'] == risk_level].sort_values(\n",
    "        'predicted_success_proba', ascending=(risk_level == 'High Risk')\n",
    "    ).head(max_students)\n",
    "    \n",
    "    student_details = students[[\n",
    "        'student_id', 'country_home', 'subject_field', 'predicted_success_proba',\n",
    "        'success_label_from_risk', 'pass_fail_prediction', 'cluster_name',\n",
    "        'gpa_sem1', 'attendance_rate', 'mean_weekly_engagement'\n",
    "    ]].copy()\n",
    "    \n",
    "    student_details.columns = ['Student_ID', 'Country', 'Subject', 'Success_Prob', \n",
    "                               'Success_Label', 'Pass/Fail', 'Cluster',\n",
    "                               'GPA', 'Attendance', 'Engagement']\n",
    "    \n",
    "    return student_details.reset_index(drop=True)\n",
    "\n",
    "# Display top students from each category\n",
    "print(\"\\nüü¢ LOW RISK STUDENTS (Top 20 by Success Probability):\")\n",
    "low_risk_students = display_student_list('Low Risk', 20)\n",
    "print(low_risk_students.to_string())\n",
    "\n",
    "print(\"\\n\\nüü° MEDIUM RISK STUDENTS (Top 20 by Success Probability):\")\n",
    "medium_risk_students = display_student_list('Medium Risk', 20)\n",
    "print(medium_risk_students.to_string())\n",
    "\n",
    "print(\"\\n\\nüî¥ HIGH RISK STUDENTS (Top 20 - Most Critical):\")\n",
    "high_risk_students = display_student_list('High Risk', 20)\n",
    "print(high_risk_students.to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# 5. RISK TRANSITION ANALYSIS (Engagement Trends)\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìà 5. ENGAGEMENT TRENDS BY RISK CATEGORY\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate engagement metrics by risk level\n",
    "engagement_trends = df_val.groupby('predicted_risk_level').agg({\n",
    "    'mean_weekly_engagement': ['mean', 'std'],\n",
    "    'low_engagement_weeks': ['mean', 'std'],\n",
    "    'attendance_rate': ['mean', 'std'],\n",
    "    'engagement_trend': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nEngagement Metrics by Risk Category:\")\n",
    "print(engagement_trends)\n",
    "\n",
    "# Visualization 5: Radar chart comparing risk categories\n",
    "from math import pi\n",
    "\n",
    "categories = ['Success Prob', 'Engagement', 'Attendance', 'GPA', 'Assignment Score']\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Calculate normalized metrics for each risk level\n",
    "risk_metrics = {}\n",
    "for risk in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "    risk_data = df_val[df_val['predicted_risk_level'] == risk]\n",
    "    risk_metrics[risk] = [\n",
    "        risk_data['predicted_success_proba'].mean(),\n",
    "        risk_data['mean_weekly_engagement'].mean(),\n",
    "        risk_data['attendance_rate'].mean(),\n",
    "        risk_data['gpa_sem1'].mean() / 10,  # Normalize to 0-1\n",
    "        risk_data['avg_assignment_score'].mean() / 100  # Normalize to 0-1\n",
    "    ]\n",
    "\n",
    "angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "colors_radar = {'Low Risk': '#27ae60', 'Medium Risk': '#f39c12', 'High Risk': '#c0392b'}\n",
    "\n",
    "for risk, values in risk_metrics.items():\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=risk, color=colors_radar[risk])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[risk])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Performance Profile Comparison by Risk Category', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. INTERACTIVE SUMMARY TABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìä 6. COMPREHENSIVE SUMMARY TABLE\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary_data = []\n",
    "\n",
    "for risk_level in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "    risk_data = df_val[df_val['predicted_risk_level'] == risk_level]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Risk_Category': risk_level,\n",
    "        'Total_Students': len(risk_data),\n",
    "        'Pass_Count': (risk_data['pass_fail_prediction'] == 'PASS').sum(),\n",
    "        'Fail_Count': (risk_data['pass_fail_prediction'] == 'FAIL').sum(),\n",
    "        'Success_Rate_%': (risk_data['success_label_from_risk'] == 'Success').mean() * 100,\n",
    "        'Avg_Success_Prob': risk_data['predicted_success_proba'].mean(),\n",
    "        'Avg_GPA': risk_data['gpa_sem1'].mean(),\n",
    "        'Avg_Attendance_%': risk_data['attendance_rate'].mean() * 100,\n",
    "        'Avg_Engagement': risk_data['mean_weekly_engagement'].mean(),\n",
    "        'Low_Engagement_Weeks': risk_data['low_engagement_weeks'].mean()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).round(2)\n",
    "print(\"\\nRisk Category Summary Statistics:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# 7. EXPORT STUDENT LISTS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüíæ 7. EXPORTING STUDENT LISTS FOR INTERVENTION\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Save detailed lists to CSV\n",
    "output_dir = './outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export by risk category\n",
    "for risk_level in ['Low Risk', 'Medium Risk', 'High Risk']:\n",
    "    filename = f\"{output_dir}/students_{risk_level.replace(' ', '_').lower()}.csv\"\n",
    "    student_list = display_student_list(risk_level, max_students=1000)\n",
    "    student_list.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Exported {risk_level} students to: {filename}\")\n",
    "\n",
    "# Export pass/fail lists\n",
    "pass_students = df_val[df_val['pass_fail_prediction'] == 'PASS'][[\n",
    "    'student_id', 'country_home', 'subject_field', 'predicted_success_proba',\n",
    "    'predicted_risk_level', 'cluster_name', 'gpa_sem1', 'attendance_rate'\n",
    "]]\n",
    "pass_students.to_csv(f\"{output_dir}/students_predicted_pass.csv\", index=False)\n",
    "print(f\"‚úÖ Exported PASS students to: {output_dir}/students_predicted_pass.csv\")\n",
    "\n",
    "fail_students = df_val[df_val['pass_fail_prediction'] == 'FAIL'][[\n",
    "    'student_id', 'country_home', 'subject_field', 'predicted_success_proba',\n",
    "    'predicted_risk_level', 'cluster_name', 'gpa_sem1', 'attendance_rate'\n",
    "]]\n",
    "fail_students.to_csv(f\"{output_dir}/students_predicted_fail.csv\", index=False)\n",
    "print(f\"‚úÖ Exported FAIL students to: {output_dir}/students_predicted_fail.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" \" * 30 + \"‚úÖ RISK ANALYSIS COMPLETE ‚úÖ\")\n",
    "print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 18. BARRIER IDENTIFICATION & ROOT CAUSE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SECTION 18: BARRIER IDENTIFICATION & ROOT CAUSE ANALYSIS\n",
    "Advanced analysis to identify specific barriers and root causes for student risk\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\" \" * 20 + \"üîç BARRIER IDENTIFICATION & ROOT CAUSE ANALYSIS üîç\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BARRIER DETECTION - Identify Students with Specific Problems\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìå SECTION 1: BARRIER IDENTIFICATION\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Define barrier thresholds dynamically\n",
    "barrier_columns = {\n",
    "    'attendance_rate': {'threshold': 0.6, 'type': 'below', 'barrier_name': 'Low Attendance'},\n",
    "    'language_proficiency': {'threshold': 0.6, 'type': 'below', 'barrier_name': 'Language Difficulty'},\n",
    "    'cultural_distance': {'threshold': 0.7, 'type': 'above', 'barrier_name': 'Cultural Adaptation Problem'},\n",
    "    'adaptability': {'threshold': 0.5, 'type': 'below', 'barrier_name': 'Poor Adaptability'},\n",
    "}\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "available_columns = df_val.columns.tolist()\n",
    "print(f\"Total columns in validation dataset: {len(available_columns)}\")\n",
    "\n",
    "# Identify barriers for each student\n",
    "barrier_flags = pd.DataFrame(index=df_val.index)\n",
    "barrier_flags['student_id'] = df_val['student_id']\n",
    "barrier_flags['predicted_risk_level'] = df_val['predicted_risk_level']\n",
    "barrier_flags['predicted_success_proba'] = df_val['predicted_success_proba']\n",
    "\n",
    "print(\"\\nüö® Detecting Barriers Based on Thresholds:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col, config in barrier_columns.items():\n",
    "    if col in available_columns:\n",
    "        threshold = config['threshold']\n",
    "        barrier_type = config['type']\n",
    "        barrier_name = config['barrier_name']\n",
    "        \n",
    "        if barrier_type == 'below':\n",
    "            barrier_flags[f'has_{col}_barrier'] = (df_val[col] < threshold).astype(int)\n",
    "        else:  # above\n",
    "            barrier_flags[f'has_{col}_barrier'] = (df_val[col] > threshold).astype(int)\n",
    "        \n",
    "        count = barrier_flags[f'has_{col}_barrier'].sum()\n",
    "        pct = (count / len(df_val)) * 100\n",
    "        print(f\"‚úì {barrier_name:30s} ({col:25s}): {count:4d} students ({pct:5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"‚ö† {col:25s}: Column not found in dataset\")\n",
    "\n",
    "# Check for additional barrier indicators\n",
    "additional_checks = {\n",
    "    'support_program': {'value': 0, 'barrier_name': 'No Support Program'},\n",
    "    'participates_in_buddy_program': {'value': 0, 'barrier_name': 'No Buddy Program'},\n",
    "    'participates_in_language_course': {'value': 0, 'barrier_name': 'No Language Course'},\n",
    "}\n",
    "\n",
    "print(\"\\nüö® Detecting Program Participation Barriers:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for col, config in additional_checks.items():\n",
    "    if col in available_columns:\n",
    "        barrier_name = config['barrier_name']\n",
    "        target_value = config['value']\n",
    "        barrier_flags[f'has_{col}_barrier'] = (df_val[col] == target_value).astype(int)\n",
    "        count = barrier_flags[f'has_{col}_barrier'].sum()\n",
    "        pct = (count / len(df_val)) * 100\n",
    "        print(f\"‚úì {barrier_name:30s} ({col:25s}): {count:4d} students ({pct:5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"‚ö† {col:25s}: Column not found in dataset\")\n",
    "\n",
    "# Count total barriers per student\n",
    "barrier_col_names = [col for col in barrier_flags.columns if col.startswith('has_')]\n",
    "barrier_flags['total_barriers'] = barrier_flags[barrier_col_names].sum(axis=1)\n",
    "\n",
    "print(\"\\nüìä Barrier Summary Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Students with 0 barriers: {(barrier_flags['total_barriers'] == 0).sum()} ({(barrier_flags['total_barriers'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Students with 1 barrier:  {(barrier_flags['total_barriers'] == 1).sum()} ({(barrier_flags['total_barriers'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"Students with 2 barriers: {(barrier_flags['total_barriers'] == 2).sum()} ({(barrier_flags['total_barriers'] == 2).mean()*100:.1f}%)\")\n",
    "print(f\"Students with 3+ barriers: {(barrier_flags['total_barriers'] >= 3).sum()} ({(barrier_flags['total_barriers'] >= 3).mean()*100:.1f}%)\")\n",
    "print(f\"Maximum barriers per student: {barrier_flags['total_barriers'].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. VISUALIZATION 1: Barrier Distribution by Risk Level\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìä VISUALIZATION 1: BARRIER DISTRIBUTION BY RISK LEVEL\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Top-left: Total barriers by risk level (box plot)\n",
    "barrier_risk_data = barrier_flags[['predicted_risk_level', 'total_barriers']].copy()\n",
    "barrier_risk_data['predicted_risk_level'] = pd.Categorical(\n",
    "    barrier_risk_data['predicted_risk_level'],\n",
    "    categories=['Low Risk', 'Medium Risk', 'High Risk'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "sns.boxplot(data=barrier_risk_data, x='predicted_risk_level', y='total_barriers', \n",
    "            ax=axes[0, 0], palette=['#27ae60', '#f39c12', '#c0392b'])\n",
    "axes[0, 0].set_title('Number of Barriers by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Risk Level', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Number of Barriers', fontsize=12)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Top-right: Average barriers per risk level\n",
    "avg_barriers = barrier_flags.groupby('predicted_risk_level')['total_barriers'].mean()\n",
    "avg_barriers = avg_barriers.reindex(['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "bars = axes[0, 1].bar(avg_barriers.index, avg_barriers.values, \n",
    "                      color=['#27ae60', '#f39c12', '#c0392b'], edgecolor='black', linewidth=1.2)\n",
    "axes[0, 1].set_title('Average Number of Barriers by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Risk Level', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Average Barriers', fontsize=12)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Bottom-left: Barrier prevalence (horizontal bar chart)\n",
    "barrier_prevalence = {}\n",
    "for col in barrier_col_names:\n",
    "    barrier_name = col.replace('has_', '').replace('_barrier', '').replace('_', ' ').title()\n",
    "    count = barrier_flags[col].sum()\n",
    "    barrier_prevalence[barrier_name] = count\n",
    "\n",
    "barrier_prev_df = pd.DataFrame(list(barrier_prevalence.items()), \n",
    "                               columns=['Barrier', 'Count']).sort_values('Count', ascending=True)\n",
    "\n",
    "axes[1, 0].barh(barrier_prev_df['Barrier'], barrier_prev_df['Count'], \n",
    "                color='#e74c3c', edgecolor='black', linewidth=1.2)\n",
    "axes[1, 0].set_title('Prevalence of Each Barrier Type', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Students Affected', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Barrier Type', fontsize=12)\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (barrier, count) in enumerate(zip(barrier_prev_df['Barrier'], barrier_prev_df['Count'])):\n",
    "    axes[1, 0].text(count + 2, i, str(count), va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Bottom-right: Correlation between barriers and success probability\n",
    "axes[1, 1].scatter(barrier_flags['total_barriers'], barrier_flags['predicted_success_proba'],\n",
    "                  alpha=0.6, c=barrier_flags['total_barriers'], cmap='RdYlGn_r', \n",
    "                  s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[1, 1].set_title('Barriers vs Success Probability', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Number of Barriers', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Predicted Success Probability', fontsize=12)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(barrier_flags['total_barriers'], barrier_flags['predicted_success_proba'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(barrier_flags['total_barriers'].min(), barrier_flags['total_barriers'].max(), 100)\n",
    "axes[1, 1].plot(x_trend, p(x_trend), \"r--\", linewidth=2, label=f'Trend: y={z[0]:.3f}x+{z[1]:.3f}')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ROOT CAUSE ANALYSIS - Identify Main Reasons for High Risk\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüî¨ SECTION 2: ROOT CAUSE ANALYSIS FOR HIGH RISK STUDENTS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Focus on High Risk students\n",
    "high_risk_students = df_val[df_val['predicted_risk_level'] == 'High Risk'].copy()\n",
    "print(f\"\\nTotal High Risk Students: {len(high_risk_students)}\")\n",
    "\n",
    "# Define all potential root cause features\n",
    "root_cause_features = [\n",
    "    # Academic performance\n",
    "    'gpa_sem1', 'gpa_sem2', 'gpa_prev', 'avg_exam_score', 'avg_assignment_score',\n",
    "    \n",
    "    # Engagement\n",
    "    'attendance_rate', 'mean_weekly_engagement', 'low_engagement_weeks', \n",
    "    'engagement_trend', 'missing_assignments_count',\n",
    "    \n",
    "    # Academic outcomes\n",
    "    'failed_courses_sem1', 'failed_courses_sem2', 'late_submission_rate',\n",
    "    \n",
    "    # Barriers\n",
    "    'language_proficiency', 'cultural_distance', 'teaching_style_difference', 'adaptability',\n",
    "    \n",
    "    # Support systems\n",
    "    'support_program', 'participates_in_buddy_program', 'participates_in_language_course',\n",
    "    \n",
    "    # Personal factors\n",
    "    'age', 'marital_status'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "available_features = [col for col in root_cause_features if col in high_risk_students.columns]\n",
    "print(f\"Analyzing {len(available_features)} available features\")\n",
    "\n",
    "# Calculate correlations with success probability for High Risk students\n",
    "correlations = {}\n",
    "for feature in available_features:\n",
    "    if high_risk_students[feature].dtype in ['int64', 'float64']:\n",
    "        corr = high_risk_students[feature].corr(high_risk_students['predicted_success_proba'])\n",
    "        correlations[feature] = corr\n",
    "\n",
    "# Sort by absolute correlation\n",
    "corr_df = pd.DataFrame(list(correlations.items()), columns=['Feature', 'Correlation'])\n",
    "corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()\n",
    "corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "print(\"\\nüìà Top 15 Features Correlated with Success Probability (High Risk Students):\")\n",
    "print(\"-\" * 100)\n",
    "print(corr_df.head(15).to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VISUALIZATION 2: Root Cause Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìä VISUALIZATION 2: ROOT CAUSE CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# Top-left: Top 10 correlated features (bar chart)\n",
    "top_features = corr_df.head(10)\n",
    "colors_corr = ['#27ae60' if x > 0 else '#e74c3c' for x in top_features['Correlation']]\n",
    "\n",
    "axes[0, 0].barh(top_features['Feature'], top_features['Correlation'], \n",
    "                color=colors_corr, edgecolor='black', linewidth=1.2)\n",
    "axes[0, 0].set_title('Top 10 Features Correlated with Success (High Risk)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Correlation with Success Probability', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Feature', fontsize=12)\n",
    "axes[0, 0].axvline(0, color='black', linewidth=1)\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Top-right: Feature importance from comparison (High Risk vs Low Risk)\n",
    "if len(df_val[df_val['predicted_risk_level'] == 'Low Risk']) > 0:\n",
    "    low_risk_students = df_val[df_val['predicted_risk_level'] == 'Low Risk'].copy()\n",
    "    \n",
    "    # Calculate mean differences\n",
    "    feature_differences = {}\n",
    "    for feature in available_features:\n",
    "        if high_risk_students[feature].dtype in ['int64', 'float64']:\n",
    "            high_mean = high_risk_students[feature].mean()\n",
    "            low_mean = low_risk_students[feature].mean()\n",
    "            diff = high_mean - low_mean\n",
    "            feature_differences[feature] = diff\n",
    "    \n",
    "    diff_df = pd.DataFrame(list(feature_differences.items()), columns=['Feature', 'Difference'])\n",
    "    diff_df['Abs_Difference'] = diff_df['Difference'].abs()\n",
    "    diff_df = diff_df.sort_values('Abs_Difference', ascending=False).head(10)\n",
    "    \n",
    "    colors_diff = ['#e74c3c' if x > 0 else '#27ae60' for x in diff_df['Difference']]\n",
    "    \n",
    "    axes[0, 1].barh(diff_df['Feature'], diff_df['Difference'], \n",
    "                    color=colors_diff, edgecolor='black', linewidth=1.2)\n",
    "    axes[0, 1].set_title('High Risk vs Low Risk: Mean Difference', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Difference (High Risk - Low Risk)', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Feature', fontsize=12)\n",
    "    axes[0, 1].axvline(0, color='black', linewidth=1)\n",
    "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom-left: Categorical root causes (if marital_status exists)\n",
    "if 'marital_status' in available_columns:\n",
    "    marital_risk = df_val.groupby(['marital_status', 'predicted_risk_level']).size().unstack(fill_value=0)\n",
    "    if 'High Risk' in marital_risk.columns:\n",
    "        marital_risk_pct = (marital_risk['High Risk'] / marital_risk.sum(axis=1)) * 100\n",
    "        marital_risk_pct = marital_risk_pct.sort_values(ascending=False)\n",
    "        \n",
    "        axes[1, 0].bar(range(len(marital_risk_pct)), marital_risk_pct.values,\n",
    "                      color='#c0392b', edgecolor='black', linewidth=1.2)\n",
    "        axes[1, 0].set_xticks(range(len(marital_risk_pct)))\n",
    "        axes[1, 0].set_xticklabels(marital_risk_pct.index, rotation=45, ha='right')\n",
    "        axes[1, 0].set_title('High Risk % by Marital Status', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('% High Risk', fontsize=12)\n",
    "        axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(marital_risk_pct.values):\n",
    "            axes[1, 0].text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Marital Status\\nNot Available', \n",
    "                   ha='center', va='center', fontsize=14, transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('High Risk % by Marital Status', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bottom-right: Support program effectiveness\n",
    "support_cols = [col for col in ['support_program', 'participates_in_buddy_program', \n",
    "                                 'participates_in_language_course'] if col in available_columns]\n",
    "\n",
    "if len(support_cols) > 0:\n",
    "    support_effectiveness = {}\n",
    "    for col in support_cols:\n",
    "        with_support = df_val[df_val[col] == 1]['predicted_success_proba'].mean()\n",
    "        without_support = df_val[df_val[col] == 0]['predicted_success_proba'].mean()\n",
    "        improvement = with_support - without_support\n",
    "        support_effectiveness[col.replace('participates_in_', '').replace('_', ' ').title()] = improvement\n",
    "    \n",
    "    support_df = pd.DataFrame(list(support_effectiveness.items()), \n",
    "                             columns=['Program', 'Success_Improvement'])\n",
    "    support_df = support_df.sort_values('Success_Improvement', ascending=False)\n",
    "    \n",
    "    colors_support = ['#27ae60' if x > 0 else '#e74c3c' for x in support_df['Success_Improvement']]\n",
    "    \n",
    "    axes[1, 1].barh(support_df['Program'], support_df['Success_Improvement'],\n",
    "                   color=colors_support, edgecolor='black', linewidth=1.2)\n",
    "    axes[1, 1].set_title('Support Program Effectiveness', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Success Probability Improvement', fontsize=12)\n",
    "    axes[1, 1].axvline(0, color='black', linewidth=1)\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (prog, imp) in enumerate(zip(support_df['Program'], support_df['Success_Improvement'])):\n",
    "        axes[1, 1].text(imp + 0.005 if imp > 0 else imp - 0.005, i, \n",
    "                       f'{imp:+.3f}', va='center', ha='left' if imp > 0 else 'right',\n",
    "                       fontsize=10, fontweight='bold')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Support Programs\\nNot Available', \n",
    "                   ha='center', va='center', fontsize=14, transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Support Program Effectiveness', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. HIGH RISK STUDENT PROFILES WITH ROOT CAUSES\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìã SECTION 3: HIGH RISK STUDENT PROFILES WITH IDENTIFIED ROOT CAUSES\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Merge barrier flags with high risk students\n",
    "high_risk_with_barriers = high_risk_students.merge(\n",
    "    barrier_flags[['student_id', 'total_barriers'] + barrier_col_names],\n",
    "    on='student_id'\n",
    ")\n",
    "\n",
    "# Identify primary root causes for each student\n",
    "def identify_root_causes(row):\n",
    "    causes = []\n",
    "    \n",
    "    # Check each barrier\n",
    "    if 'has_attendance_rate_barrier' in row.index and row['has_attendance_rate_barrier'] == 1:\n",
    "        causes.append('Low Attendance')\n",
    "    if 'has_language_proficiency_barrier' in row.index and row['has_language_proficiency_barrier'] == 1:\n",
    "        causes.append('Language Difficulty')\n",
    "    if 'has_cultural_distance_barrier' in row.index and row['has_cultural_distance_barrier'] == 1:\n",
    "        causes.append('Cultural Adaptation')\n",
    "    if 'has_adaptability_barrier' in row.index and row['has_adaptability_barrier'] == 1:\n",
    "        causes.append('Poor Adaptability')\n",
    "    if 'has_support_program_barrier' in row.index and row['has_support_program_barrier'] == 1:\n",
    "        causes.append('No Support Program')\n",
    "    if 'has_participates_in_buddy_program_barrier' in row.index and row['has_participates_in_buddy_program_barrier'] == 1:\n",
    "        causes.append('No Buddy Program')\n",
    "    if 'has_participates_in_language_course_barrier' in row.index and row['has_participates_in_language_course_barrier'] == 1:\n",
    "        causes.append('No Language Course')\n",
    "    \n",
    "    # Check academic issues\n",
    "    if 'low_engagement_weeks' in row.index and row['low_engagement_weeks'] >= 3:\n",
    "        causes.append('High Disengagement')\n",
    "    if 'failed_courses_sem1' in row.index and row['failed_courses_sem1'] >= 2:\n",
    "        causes.append('Multiple Failures')\n",
    "    if 'gpa_sem1' in row.index and row['gpa_sem1'] < 5.0:\n",
    "        causes.append('Low GPA')\n",
    "    \n",
    "    return ', '.join(causes) if causes else 'No Clear Barriers Identified'\n",
    "\n",
    "high_risk_with_barriers['identified_root_causes'] = high_risk_with_barriers.apply(identify_root_causes, axis=1)\n",
    "\n",
    "# Select relevant columns for display\n",
    "display_cols = ['student_id', 'country_home', 'subject_field', 'predicted_success_proba', \n",
    "                'total_barriers', 'identified_root_causes']\n",
    "\n",
    "# Add available barrier-related columns\n",
    "for col in ['attendance_rate', 'language_proficiency', 'cultural_distance', 'adaptability',\n",
    "            'gpa_sem1', 'mean_weekly_engagement']:\n",
    "    if col in high_risk_with_barriers.columns:\n",
    "        display_cols.append(col)\n",
    "\n",
    "high_risk_profile = high_risk_with_barriers[display_cols].copy()\n",
    "high_risk_profile = high_risk_profile.sort_values('predicted_success_proba').head(30)\n",
    "\n",
    "print(\"\\nüö® Top 30 Most Critical High Risk Students with Root Causes:\")\n",
    "print(\"-\" * 100)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "print(high_risk_profile.to_string(index=False))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ROOT CAUSE FREQUENCY ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìä SECTION 4: ROOT CAUSE FREQUENCY ANALYSIS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Count frequency of each root cause\n",
    "all_causes = []\n",
    "for causes_str in high_risk_with_barriers['identified_root_causes']:\n",
    "    if causes_str != 'No Clear Barriers Identified':\n",
    "        all_causes.extend([c.strip() for c in causes_str.split(',')])\n",
    "\n",
    "cause_counts = pd.Series(all_causes).value_counts()\n",
    "\n",
    "print(\"\\nüî• Most Common Root Causes Among High Risk Students:\")\n",
    "print(\"-\" * 100)\n",
    "for i, (cause, count) in enumerate(cause_counts.items(), 1):\n",
    "    pct = (count / len(high_risk_students)) * 100\n",
    "    print(f\"{i:2d}. {cause:35s}: {count:4d} students ({pct:5.1f}% of high risk)\")\n",
    "\n",
    "# Visualization: Root cause frequency\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.barh(range(len(cause_counts)), cause_counts.values, color='#e74c3c', edgecolor='black', linewidth=1.2)\n",
    "ax.set_yticks(range(len(cause_counts)))\n",
    "ax.set_yticklabels(cause_counts.index)\n",
    "ax.set_xlabel('Number of High Risk Students Affected', fontsize=12)\n",
    "ax.set_title('Most Common Root Causes for High Risk Students', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (cause, count) in enumerate(cause_counts.items()):\n",
    "    pct = (count / len(high_risk_students)) * 100\n",
    "    ax.text(count + 0.5, i, f'{count} ({pct:.1f}%)', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. EXPORT BARRIER AND ROOT CAUSE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüíæ SECTION 5: EXPORTING BARRIER AND ROOT CAUSE DATA\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Export high risk students with root causes\n",
    "output_file = './outputs/high_risk_students_with_root_causes.csv'\n",
    "high_risk_with_barriers[display_cols].to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported: {output_file}\")\n",
    "print(f\"   Contains: {len(high_risk_with_barriers)} high risk students with identified barriers and root causes\")\n",
    "\n",
    "# Export barrier summary for all students\n",
    "barrier_summary = df_val[['student_id', 'predicted_risk_level', 'predicted_success_proba']].copy()\n",
    "barrier_summary = barrier_summary.merge(barrier_flags[['student_id', 'total_barriers'] + barrier_col_names], \n",
    "                                       on='student_id')\n",
    "\n",
    "output_file2 = './outputs/all_students_barrier_analysis.csv'\n",
    "barrier_summary.to_csv(output_file2, index=False)\n",
    "print(f\"‚úÖ Exported: {output_file2}\")\n",
    "print(f\"   Contains: {len(barrier_summary)} students with barrier flags\")\n",
    "\n",
    "# Export root cause summary\n",
    "root_cause_summary = pd.DataFrame({\n",
    "    'Root_Cause': cause_counts.index,\n",
    "    'Count': cause_counts.values,\n",
    "    'Percentage_of_High_Risk': (cause_counts.values / len(high_risk_students) * 100).round(1)\n",
    "})\n",
    "\n",
    "output_file3 = './outputs/root_cause_frequency.csv'\n",
    "root_cause_summary.to_csv(output_file3, index=False)\n",
    "print(f\"‚úÖ Exported: {output_file3}\")\n",
    "print(f\"   Contains: Frequency analysis of root causes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" \" * 25 + \"‚úÖ BARRIER & ROOT CAUSE ANALYSIS COMPLETE ‚úÖ\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SECTION 19: UNDERSTANDING K-MEANS CLUSTERING - EDUCATIONAL WALKTHROUGH\n",
    "\n",
    "This section explains HOW the 4 student clusters were created and \n",
    "provides tools to explore and understand cluster membership.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\" \" * 25 + \"üî¨ K-MEANS CLUSTERING EXPLAINED üî¨\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: WHAT IS K-MEANS CLUSTERING?\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìö PART 1: UNDERSTANDING K-MEANS CLUSTERING\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "K-Means Clustering is an UNSUPERVISED learning algorithm that:\n",
    "  1. Groups similar students together based on their features\n",
    "  2. Creates K groups (we use K=4) without being told what \"good\" or \"bad\" is\n",
    "  3. Finds natural patterns in the data\n",
    "\n",
    "Think of it like organizing a party:\n",
    "  ‚Ä¢ You have 356 guests (students)\n",
    "  ‚Ä¢ You want to seat them at 4 tables\n",
    "  ‚Ä¢ You want similar people at each table\n",
    "  ‚Ä¢ K-Means figures out the best seating arrangement!\n",
    "\n",
    "Our 4 clusters represent:\n",
    "  üåü Cluster 1: Elite Performers       (Best students, high success)\n",
    "  üí™ Cluster 2: Strong Achievers       (Good students, solid performance)\n",
    "  ‚ö†Ô∏è  Cluster 3: Moderate Performers   (Average students, could go either way)\n",
    "  üö® Cluster 4: At-Risk Students       (Struggling students, need help)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: THE FEATURES USED FOR CLUSTERING\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìä PART 2: WHICH FEATURES ARE USED TO CREATE CLUSTERS?\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Define the features we use for clustering\n",
    "cluster_features = [\n",
    "    'predicted_success_proba',      # Overall success likelihood (0-1)\n",
    "    'mean_weekly_engagement',       # Weekly engagement score (0-1)\n",
    "    'attendance_rate',              # Class attendance percentage (0-1)\n",
    "    'avg_assignment_score',         # Average assignment score (0-100)\n",
    "    'avg_exam_score',               # Average exam score (0-100)\n",
    "    'gpa_sem1',                     # Semester 1 GPA (0-10)\n",
    "    'gpa_sem2',                     # Semester 2 GPA (0-10)\n",
    "    'low_engagement_weeks',         # Number of weeks with low engagement\n",
    "    'failed_courses_sem1',          # Failed courses in semester 1\n",
    "    'failed_courses_sem2'           # Failed courses in semester 2\n",
    "]\n",
    "\n",
    "print(\"We use 10 features to group students:\")\n",
    "print()\n",
    "for i, feature in enumerate(cluster_features, 1):\n",
    "    if feature in df_val.columns:\n",
    "        mean_val = df_val[feature].mean()\n",
    "        std_val = df_val[feature].std()\n",
    "        print(f\"{i:2d}. {feature:30s} ‚Üí Mean: {mean_val:7.3f}, Std: {std_val:7.3f}\")\n",
    "    else:\n",
    "        print(f\"{i:2d}. {feature:30s} ‚Üí NOT AVAILABLE in dataset\")\n",
    "\n",
    "print(\"\\nüìå Why these features?\")\n",
    "print(\"   ‚Ä¢ Academic performance: GPA, exam scores, assignment scores\")\n",
    "print(\"   ‚Ä¢ Behavioral patterns: Engagement, attendance\")\n",
    "print(\"   ‚Ä¢ Outcomes: Failed courses, success probability\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: STEP-BY-STEP CLUSTERING PROCESS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüîß PART 3: HOW K-MEANS CREATES THE 4 CLUSTERS (STEP-BY-STEP)\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check if clustering was already done in Section 17\n",
    "if 'cluster_label' not in df_val.columns or 'cluster_name' not in df_val.columns:\n",
    "    print(\"\\n‚ö†Ô∏è  Clustering not found in df_val. Running K-Means now...\\n\")\n",
    "    \n",
    "    # STEP 1: Extract features\n",
    "    print(\"STEP 1: Extracting features from validation dataset...\")\n",
    "    available_features = [f for f in cluster_features if f in df_val.columns]\n",
    "    X_cluster = df_val[available_features].copy()\n",
    "    print(f\"   ‚úì Using {len(available_features)} features\")\n",
    "    print(f\"   ‚úì Dataset size: {X_cluster.shape[0]} students √ó {X_cluster.shape[1]} features\")\n",
    "    \n",
    "    # STEP 2: Standardize features\n",
    "    print(\"\\nSTEP 2: Standardizing features (making them comparable)...\")\n",
    "    print(\"   Why? GPA is 0-10, but engagement is 0-1. Need same scale!\")\n",
    "    scaler = StandardScaler()\n",
    "    X_cluster_scaled = scaler.fit_transform(X_cluster)\n",
    "    print(f\"   ‚úì All features now have mean=0, std=1\")\n",
    "    \n",
    "    # STEP 3: Run K-Means\n",
    "    print(\"\\nSTEP 3: Running K-Means algorithm with k=4 clusters...\")\n",
    "    print(\"   The algorithm:\")\n",
    "    print(\"   1. Randomly places 4 'center points' in the data\")\n",
    "    print(\"   2. Assigns each student to nearest center\")\n",
    "    print(\"   3. Moves centers to middle of their students\")\n",
    "    print(\"   4. Repeats until centers stop moving\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "    df_val['cluster_label'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "    print(f\"   ‚úì Clustering complete! Each student assigned to a cluster (0-3)\")\n",
    "    \n",
    "    # STEP 4: Name the clusters\n",
    "    print(\"\\nSTEP 4: Naming clusters based on success probability...\")\n",
    "    cluster_means = df_val.groupby('cluster_label')['predicted_success_proba'].mean()\n",
    "    cluster_means = cluster_means.sort_values(ascending=False)\n",
    "    \n",
    "    cluster_mapping = {\n",
    "        cluster_means.index[0]: 'Elite Performers',\n",
    "        cluster_means.index[1]: 'Strong Achievers',\n",
    "        cluster_means.index[2]: 'Moderate Performers',\n",
    "        cluster_means.index[3]: 'At-Risk Students'\n",
    "    }\n",
    "    \n",
    "    df_val['cluster_name'] = df_val['cluster_label'].map(cluster_mapping)\n",
    "    print(f\"   ‚úì Clusters named meaningfully:\")\n",
    "    for label, name in cluster_mapping.items():\n",
    "        count = (df_val['cluster_label'] == label).sum()\n",
    "        avg_prob = df_val[df_val['cluster_label'] == label]['predicted_success_proba'].mean()\n",
    "        print(f\"      Cluster {label} ‚Üí {name:25s} (n={count:3d}, avg_success={avg_prob:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úì Clustering already exists in df_val (created in Section 17)\")\n",
    "    print(\"  Using existing cluster_label and cluster_name columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: CLUSTER STATISTICS - UNDERSTANDING EACH CLUSTER\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìà PART 4: DETAILED STATISTICS FOR EACH CLUSTER\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate comprehensive statistics for each cluster\n",
    "print(\"\\nüéØ Cluster Profiles:\")\n",
    "print()\n",
    "\n",
    "for cluster_name in ['Elite Performers', 'Strong Achievers', 'Moderate Performers', 'At-Risk Students']:\n",
    "    cluster_data = df_val[df_val['cluster_name'] == cluster_name]\n",
    "    \n",
    "    if len(cluster_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"üè∑Ô∏è  {cluster_name.upper()}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"   Count: {len(cluster_data)} students ({len(cluster_data)/len(df_val)*100:.1f}% of total)\")\n",
    "    print()\n",
    "    print(\"   Key Metrics:\")\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    metrics = {\n",
    "        'Success Probability': 'predicted_success_proba',\n",
    "        'Weekly Engagement': 'mean_weekly_engagement',\n",
    "        'Attendance Rate': 'attendance_rate',\n",
    "        'GPA (Sem 1)': 'gpa_sem1',\n",
    "        'Avg Assignment Score': 'avg_assignment_score',\n",
    "        'Avg Exam Score': 'avg_exam_score',\n",
    "        'Low Engagement Weeks': 'low_engagement_weeks',\n",
    "        'Failed Courses (Sem 1)': 'failed_courses_sem1'\n",
    "    }\n",
    "    \n",
    "    for metric_name, column in metrics.items():\n",
    "        if column in cluster_data.columns:\n",
    "            mean_val = cluster_data[column].mean()\n",
    "            std_val = cluster_data[column].std()\n",
    "            min_val = cluster_data[column].min()\n",
    "            max_val = cluster_data[column].max()\n",
    "            print(f\"   ‚Ä¢ {metric_name:25s}: Mean={mean_val:6.3f}, Std={std_val:6.3f}, Range=[{min_val:6.3f}, {max_val:6.3f}]\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: HOW TO FIND A SPECIFIC STUDENT'S CLUSTER\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüîç PART 5: HOW TO FIND WHICH CLUSTER A STUDENT BELONGS TO\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"\\nMethod 1: Look up by Student ID\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example: Find first 5 students' clusters\n",
    "sample_students = df_val[['student_id', 'cluster_name', 'predicted_success_proba', \n",
    "                          'mean_weekly_engagement', 'gpa_sem1']].head(5)\n",
    "\n",
    "print(\"\\nExample - First 5 students:\")\n",
    "print(sample_students.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nMethod 2: Find all students in a specific cluster\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "elite_students = df_val[df_val['cluster_name'] == 'Elite Performers'][\n",
    "    ['student_id', 'predicted_success_proba', 'gpa_sem1', 'attendance_rate']\n",
    "].head(10)\n",
    "\n",
    "print(\"\\nExample - First 10 Elite Performers:\")\n",
    "print(elite_students.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nMethod 3: Count students in each cluster\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cluster_counts = df_val['cluster_name'].value_counts().sort_index()\n",
    "print(\"\\nStudent distribution across clusters:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    pct = count / len(df_val) * 100\n",
    "    print(f\"   {cluster:25s}: {count:4d} students ({pct:5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: INTERACTIVE CLUSTER LOOKUP FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüíª PART 6: INTERACTIVE FUNCTIONS TO EXPLORE CLUSTERS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "def get_student_cluster(student_id):\n",
    "    \"\"\"\n",
    "    Look up which cluster a student belongs to\n",
    "    \n",
    "    Args:\n",
    "        student_id: The student ID to look up\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with cluster information\n",
    "    \"\"\"\n",
    "    student_data = df_val[df_val['student_id'] == student_id]\n",
    "    \n",
    "    if len(student_data) == 0:\n",
    "        return f\"Student {student_id} not found in validation dataset\"\n",
    "    \n",
    "    student = student_data.iloc[0]\n",
    "    \n",
    "    info = {\n",
    "        'Student ID': student_id,\n",
    "        'Cluster': student['cluster_name'],\n",
    "        'Cluster Number': student['cluster_label'],\n",
    "        'Success Probability': student['predicted_success_proba'],\n",
    "        'Risk Level': student['predicted_risk_level'],\n",
    "        'GPA': student['gpa_sem1'] if 'gpa_sem1' in student.index else 'N/A',\n",
    "        'Attendance': student['attendance_rate'] if 'attendance_rate' in student.index else 'N/A',\n",
    "        'Engagement': student['mean_weekly_engagement'] if 'mean_weekly_engagement' in student.index else 'N/A'\n",
    "    }\n",
    "    \n",
    "    return info\n",
    "\n",
    "def get_cluster_summary(cluster_name):\n",
    "    \"\"\"\n",
    "    Get summary statistics for a specific cluster\n",
    "    \n",
    "    Args:\n",
    "        cluster_name: Name of cluster ('Elite Performers', 'Strong Achievers', etc.)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with summary statistics\n",
    "    \"\"\"\n",
    "    cluster_data = df_val[df_val['cluster_name'] == cluster_name]\n",
    "    \n",
    "    if len(cluster_data) == 0:\n",
    "        return f\"Cluster '{cluster_name}' not found\"\n",
    "    \n",
    "    summary = cluster_data.describe().T\n",
    "    return summary\n",
    "\n",
    "def compare_clusters():\n",
    "    \"\"\"\n",
    "    Compare all clusters side-by-side\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame comparing key metrics across clusters\n",
    "    \"\"\"\n",
    "    comparison = df_val.groupby('cluster_name').agg({\n",
    "        'student_id': 'count',\n",
    "        'predicted_success_proba': ['mean', 'std'],\n",
    "        'mean_weekly_engagement': 'mean',\n",
    "        'attendance_rate': 'mean',\n",
    "        'gpa_sem1': 'mean',\n",
    "        'failed_courses_sem1': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    comparison.columns = ['Count', 'Success_Mean', 'Success_Std', \n",
    "                          'Engagement', 'Attendance', 'GPA', 'Failed_Courses']\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Demonstrate the functions\n",
    "print(\"\\n‚úÖ Functions created! Here's how to use them:\")\n",
    "print()\n",
    "print(\"1. get_student_cluster('S12345')\")\n",
    "print(\"   ‚Üí Returns cluster info for student S12345\")\n",
    "print()\n",
    "print(\"2. get_cluster_summary('Elite Performers')\")\n",
    "print(\"   ‚Üí Returns detailed statistics for Elite Performers\")\n",
    "print()\n",
    "print(\"3. compare_clusters()\")\n",
    "print(\"   ‚Üí Returns side-by-side comparison of all clusters\")\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n\\nüìã Example: Looking up a specific student\")\n",
    "print(\"-\" * 100)\n",
    "sample_student_id = df_val['student_id'].iloc[0]\n",
    "student_info = get_student_cluster(sample_student_id)\n",
    "\n",
    "print(f\"\\nStudent Lookup Example:\")\n",
    "for key, value in student_info.items():\n",
    "    print(f\"   {key:25s}: {value}\")\n",
    "\n",
    "print(\"\\n\\nüìã Example: Comparing all clusters\")\n",
    "print(\"-\" * 100)\n",
    "comparison_table = compare_clusters()\n",
    "print(\"\\nCluster Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: VISUALIZING THE CLUSTERS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüé® PART 7: VISUALIZING THE 4 CLUSTERS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Check if we have the necessary features\n",
    "available_features = [f for f in cluster_features if f in df_val.columns]\n",
    "\n",
    "if len(available_features) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    \n",
    "    # Plot 1: PCA Scatter Plot (2D visualization of 10D data)\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    print(\"   1. PCA Scatter Plot (reducing 10 dimensions to 2D)\")\n",
    "    \n",
    "    X_cluster = df_val[available_features].copy()\n",
    "    X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
    "    \n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "    \n",
    "    cluster_colors = {\n",
    "        'Elite Performers': '#2ecc71',\n",
    "        'Strong Achievers': '#3498db',\n",
    "        'Moderate Performers': '#f39c12',\n",
    "        'At-Risk Students': '#e74c3c'\n",
    "    }\n",
    "    \n",
    "    for cluster_name in cluster_colors.keys():\n",
    "        mask = df_val['cluster_name'] == cluster_name\n",
    "        axes[0, 0].scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
    "                          label=cluster_name, alpha=0.6, s=80,\n",
    "                          color=cluster_colors[cluster_name],\n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    axes[0, 0].set_title('Cluster Visualization in PCA Space', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)\n",
    "    axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)\n",
    "    axes[0, 0].legend(title='Cluster', fontsize=9, loc='best')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cluster Size Distribution\n",
    "    print(\"   2. Cluster Size Distribution\")\n",
    "    cluster_counts = df_val['cluster_name'].value_counts()\n",
    "    \n",
    "    bars = axes[0, 1].bar(range(len(cluster_counts)), cluster_counts.values,\n",
    "                          color=[cluster_colors[name] for name in cluster_counts.index],\n",
    "                          edgecolor='black', linewidth=1.2)\n",
    "    axes[0, 1].set_xticks(range(len(cluster_counts)))\n",
    "    axes[0, 1].set_xticklabels(cluster_counts.index, rotation=45, ha='right')\n",
    "    axes[0, 1].set_title('Number of Students in Each Cluster', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Number of Students', fontsize=12)\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar, count in zip(bars, cluster_counts.values):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{count}\\n({count/len(df_val)*100:.1f}%)',\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Success Probability by Cluster\n",
    "    print(\"   3. Success Probability Distribution\")\n",
    "    \n",
    "    cluster_order = ['Elite Performers', 'Strong Achievers', 'Moderate Performers', 'At-Risk Students']\n",
    "    df_val['cluster_name_ordered'] = pd.Categorical(df_val['cluster_name'], \n",
    "                                                     categories=cluster_order, \n",
    "                                                     ordered=True)\n",
    "    \n",
    "    sns.boxplot(data=df_val, x='cluster_name_ordered', y='predicted_success_proba',\n",
    "                ax=axes[1, 0], palette=[cluster_colors[c] for c in cluster_order])\n",
    "    axes[1, 0].set_title('Success Probability Distribution by Cluster', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Cluster', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Predicted Success Probability', fontsize=12)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Feature Comparison Across Clusters\n",
    "    print(\"   4. Feature Comparison Heatmap\")\n",
    "    \n",
    "    feature_comparison = df_val.groupby('cluster_name')[\n",
    "        ['predicted_success_proba', 'mean_weekly_engagement', 'attendance_rate', \n",
    "         'gpa_sem1', 'avg_assignment_score']\n",
    "    ].mean()\n",
    "    \n",
    "    feature_comparison = feature_comparison.reindex(cluster_order)\n",
    "    \n",
    "    # Normalize for heatmap\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    feature_comparison_norm = pd.DataFrame(\n",
    "        MinMaxScaler().fit_transform(feature_comparison),\n",
    "        index=feature_comparison.index,\n",
    "        columns=['Success Prob', 'Engagement', 'Attendance', 'GPA', 'Assignments']\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(feature_comparison_norm.T, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                cbar_kws={'label': 'Normalized Score (0-1)'}, ax=axes[1, 1],\n",
    "                linewidths=0.5, linecolor='black')\n",
    "    axes[1, 1].set_title('Cluster Performance Profiles', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Cluster', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Performance Metrics', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Visualizations complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not enough features available for visualization\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 8: HOW CLUSTERS RELATE TO RISK LEVELS\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüîó PART 8: HOW CLUSTERS RELATE TO RISK LEVELS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'predicted_risk_level' in df_val.columns:\n",
    "    # Cross-tabulation\n",
    "    cluster_risk_crosstab = pd.crosstab(df_val['cluster_name'], \n",
    "                                        df_val['predicted_risk_level'],\n",
    "                                        margins=True)\n",
    "    \n",
    "    print(\"\\nCluster √ó Risk Level Cross-Tabulation:\")\n",
    "    print(cluster_risk_crosstab)\n",
    "    \n",
    "    # Percentage breakdown\n",
    "    print(\"\\n\\nPercentage Breakdown (% within each cluster):\")\n",
    "    cluster_risk_pct = pd.crosstab(df_val['cluster_name'], \n",
    "                                   df_val['predicted_risk_level'],\n",
    "                                   normalize='index') * 100\n",
    "    print(cluster_risk_pct.round(1))\n",
    "    \n",
    "    print(\"\\nüìä Key Insights:\")\n",
    "    print(\"   ‚Ä¢ Elite Performers should be mostly Low Risk\")\n",
    "    print(\"   ‚Ä¢ At-Risk Students should be mostly High Risk\")\n",
    "    print(\"   ‚Ä¢ Strong/Moderate Achievers spread across Medium/Low Risk\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 9: EXPORTING CLUSTER INFORMATION\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüíæ PART 9: EXPORTING CLUSTER DATA\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Export each cluster to separate CSV\n",
    "output_dir = './outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for cluster_name in df_val['cluster_name'].unique():\n",
    "    cluster_data = df_val[df_val['cluster_name'] == cluster_name]\n",
    "    \n",
    "    # Select relevant columns\n",
    "    export_cols = ['student_id', 'cluster_name', 'predicted_risk_level',\n",
    "                   'predicted_success_proba', 'mean_weekly_engagement',\n",
    "                   'attendance_rate', 'gpa_sem1', 'country_home', 'subject_field']\n",
    "    \n",
    "    export_cols = [col for col in export_cols if col in cluster_data.columns]\n",
    "    \n",
    "    filename = f\"{output_dir}/cluster_{cluster_name.replace(' ', '_').lower()}.csv\"\n",
    "    cluster_data[export_cols].to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported {cluster_name:25s}: {len(cluster_data):4d} students ‚Üí {filename}\")\n",
    "\n",
    "# Export cluster comparison summary\n",
    "comparison_summary = compare_clusters()\n",
    "comparison_summary.to_csv(f\"{output_dir}/cluster_comparison_summary.csv\")\n",
    "print(f\"‚úÖ Exported cluster comparison summary ‚Üí {output_dir}/cluster_comparison_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" \" * 25 + \"‚úÖ K-MEANS CLUSTERING TUTORIAL COMPLETE ‚úÖ\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìö SUMMARY:\")\n",
    "print(\"   ‚Ä¢ K-Means creates 4 natural groups of similar students\")\n",
    "print(\"   ‚Ä¢ Uses 10 performance/engagement features\")\n",
    "print(\"   ‚Ä¢ Elite Performers: Top students (high success)\")\n",
    "print(\"   ‚Ä¢ Strong Achievers: Solid performers\")\n",
    "print(\"   ‚Ä¢ Moderate Performers: Average students\")\n",
    "print(\"   ‚Ä¢ At-Risk Students: Need urgent intervention\")\n",
    "print(\"\\n   Use the lookup functions above to explore your clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
