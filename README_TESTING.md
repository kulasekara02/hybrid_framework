# Testing Your Hybrid Model - Complete Guide

## üìö What You Have

I've created a complete testing framework for your hybrid LSTM + RF/GB model:

### üìÑ Documentation Files
1. **[QUICK_TEST_GUIDE.md](QUICK_TEST_GUIDE.md)** ‚ö° START HERE
   - Quick 3-step guide to test your model
   - Checklists and commands
   - Troubleshooting tips

2. **[TEST_MODEL_GUIDE.md](TEST_MODEL_GUIDE.md)** üìñ DETAILED REFERENCE
   - Comprehensive step-by-step instructions
   - Code examples for each step
   - Advanced testing techniques
   - Validation strategies

### üêç Python Scripts
3. **[test_hybrid_model.py](test_hybrid_model.py)** üéØ MAIN TEST SCRIPT
   - Automated testing pipeline
   - Loads models, preprocesses data, evaluates performance
   - Generates visualizations and reports
   - Run with: `python test_hybrid_model.py --timestamp <timestamp>`

4. **[generate_synthetic_data.py](generate_synthetic_data.py)** üìä DATA GENERATOR
   - Creates synthetic test data
   - Already exists in your project
   - Generates matching static and temporal CSV files

---

## üöÄ How to Test Your Model (Quick Start)

### Option 1: Using Existing Test Data
```bash
# Step 1: Find your latest model
ls results/lstm_model_*.h5 | tail -1

# Step 2: Find your latest test data
ls uploads/synthetic_*.csv | tail -2

# Step 3: Run test (update timestamps to match your files)
python test_hybrid_model.py \
    --timestamp 20260111_023557 \
    --static uploads/synthetic_static_20260111_023313.csv \
    --temporal uploads/synthetic_temporal_20260111_023313.csv
```

### Option 2: Generate Fresh Test Data
```bash
# Automatically generates new test data and tests
python test_hybrid_model.py --timestamp 20260111_023557 --generate
```

### Option 3: List Available Models First
```bash
# See all trained models
python test_hybrid_model.py --list

# Then test with desired timestamp
python test_hybrid_model.py --timestamp <chosen_timestamp> --generate
```

---

## üìã What You Need to Test

### Required Model Files (same timestamp)
Your latest model is from: **20260111_023557**

Check these files exist in `results/`:
```
‚úì lstm_model_20260111_023557.h5              (LSTM neural network)
‚úì rf_model_20260111_023557.pkl               (Random Forest)
‚úì gb_model_20260111_023557.pkl               (Gradient Boosting)
‚úì meta_learner_20260111_023557.pkl           (Combines predictions)
‚úì preprocessing_objects_20260111_023557.pkl  (Scalers & encoders)
```

Verify:
```bash
ls -lh results/*20260111_023557*
```

### Required Test Data Files
Any pair of matching static + temporal CSV files from `uploads/`:
```
‚úì synthetic_static_YYYYMMDD_HHMMSS.csv    (student features)
‚úì synthetic_temporal_YYYYMMDD_HHMMSS.csv  (weekly engagement)
```

Verify:
```bash
ls uploads/synthetic_*.csv | tail -4
```

---

## üéØ What the Test Does

The test script performs 8 steps:

1. **Load Models** - Loads all 5 model components
2. **Load Data** - Reads static and temporal CSV files
3. **Preprocess** - Scales features, encodes categories, creates sequences
4. **Predict** - Generates predictions from LSTM, RF, GB, and Hybrid
5. **Evaluate** - Calculates accuracy, precision, recall, F1, ROC-AUC
6. **Threshold Analysis** - Tests different probability thresholds
7. **Visualize** - Creates ROC curves, confusion matrix, distributions
8. **Report** - Saves text report with all metrics

### Output Files
After testing, you'll get:
- `test_results_roc_<timestamp>.png` - ROC curve comparison
- `test_results_confusion_matrix_<timestamp>.png` - Confusion matrix heatmap
- `test_results_distributions_<timestamp>.png` - Prediction distributions
- `test_results_report_<timestamp>.txt` - Complete metrics report

---

## ‚úÖ Expected Performance

### Good Performance (Model is Working)
- Accuracy: **75-85%**
- Precision: **70-80%**
- Recall: **70-85%**
- F1-Score: **70-80%**
- ROC-AUC: **80-90%**

### Key Indicator
**Hybrid model should outperform individual models by 3-7% in F1-score**

Example:
```
Model              Accuracy     F1-Score     ROC-AUC
-----------------------------------------------------
LSTM               0.7842       0.7781       0.8523
Random Forest      0.8123       0.8110       0.8742
Gradient Boost     0.8234       0.8227       0.8856
HYBRID (Best!)     0.8567       0.8548       0.9123  ‚Üê Should be highest
```

---

## üìä Understanding Your Results

### Confusion Matrix Explained
```
                  Predicted
                  Fail    Success
Actual  Fail    [ 180]   [ 45]     ‚Üê 180 correct, 45 false alarms
        Success [ 32]    [243]     ‚Üê 32 missed, 243 correct
```

**For Student Success Prediction:**
- **False Negatives (FN = 32)**: Students we predicted would fail but actually succeeded
  - Less critical - we provided extra help unnecessarily
- **False Positives (FP = 45)**: Students we predicted would succeed but failed
  - **More critical** - we missed at-risk students who needed help!

**Want to minimize False Positives** (catching at-risk students is priority)

### Metrics Interpretation

**Accuracy (85.7%)**
- Out of 500 students, we correctly predicted 428
- Good overall but doesn't tell full story

**Precision (84.2%)**
- Of students we predicted would succeed, 84.2% actually did
- 15.8% false success predictions

**Recall (86.8%)**
- Of students who actually succeeded, we predicted 86.8%
- We missed 13.2% of successful students

**F1-Score (85.5%)**
- Balanced measure combining precision and recall
- Best metric for comparing models

**ROC-AUC (91.2%)**
- Excellent discrimination ability
- Model can distinguish success/failure well

---

## üîç Testing Workflow

### First Time Testing
1. Read [QUICK_TEST_GUIDE.md](QUICK_TEST_GUIDE.md)
2. Run pre-test checklist
3. Execute test script with your latest model
4. Review console output
5. Check generated visualizations
6. Read the text report

### Regular Testing
```bash
# Quick test with latest model
python test_hybrid_model.py --list  # Find latest
python test_hybrid_model.py --timestamp <latest> --generate
```

### Before Deployment
1. Test on multiple datasets (3-5 different test sets)
2. Check performance consistency
3. Verify hybrid outperforms individual models
4. Review false positive/negative cases
5. Document all results

---

## üêõ Common Issues & Solutions

### ‚ùå "FileNotFoundError: lstm_model_*.h5"
**Problem**: Model timestamp doesn't match or files missing
**Solution**:
```bash
# Check what models exist
python test_hybrid_model.py --list

# Use exact timestamp shown
python test_hybrid_model.py --timestamp 20260111_023557 --generate
```

### ‚ùå "ValueError: Shape mismatch"
**Problem**: Test data has different features than training data
**Solution**: Use `generate_synthetic_data.py` to create compatible test data
```bash
python generate_synthetic_data.py
# Then use the newly generated files
```

### ‚ùå Low Performance (Accuracy < 60%)
**Problem**: Model not generalizing well
**Solution**:
1. Check if all model files are from same timestamp
2. Generate fresh test data: `python generate_synthetic_data.py`
3. Verify test data isn't corrupted
4. Model may need retraining

### ‚ùå "No module named 'tensorflow'"
**Problem**: Missing dependencies
**Solution**:
```bash
# Install required packages
pip install tensorflow scikit-learn pandas numpy matplotlib seaborn joblib

# Or if you have a requirements.txt
pip install -r requirements.txt
```

---

## üìà Next Steps After Testing

### If Performance is Good (‚â•75% accuracy)
1. ‚úì Document results
2. ‚úì Test on multiple datasets for consistency
3. ‚úì Consider deployment
4. ‚úì Set up monitoring for production

### If Performance is Poor (<70% accuracy)
1. ‚ö† Review training data quality
2. ‚ö† Check for data leakage
3. ‚ö† Try different hyperparameters
4. ‚ö† Collect more training data
5. ‚ö† Retrain model

### If Hybrid ‚â§ Individual Models
1. ‚ö† Meta-learner may need adjustment
2. ‚ö† Try different meta-learner algorithms
3. ‚ö† Check if predictions are correlated
4. ‚ö† Review stacking architecture

---

## üí° Pro Tips

### Testing Best Practices
1. **Never test on training data** - Use unseen data only
2. **Test multiple times** - Generate 3-5 different test sets
3. **Check consistency** - Results should be similar across test sets
4. **Save everything** - Keep all visualizations and reports
5. **Compare with baseline** - Track improvement over time

### Interpreting Results
1. **Don't just look at accuracy** - F1 and AUC are more informative
2. **Check confusion matrix** - Understand where errors occur
3. **Review distributions** - See prediction confidence
4. **Compare all models** - Ensure hybrid adds value
5. **Consider context** - 85% accuracy means helping 425/500 students correctly!

### For Production Deployment
1. Test on data from different time periods
2. Test on different student populations
3. Validate with domain experts
4. Set up monitoring dashboards
5. Plan for model updates

---

## üìû Need Help?

### Quick Reference
- **Quick start**: See [QUICK_TEST_GUIDE.md](QUICK_TEST_GUIDE.md)
- **Detailed steps**: See [TEST_MODEL_GUIDE.md](TEST_MODEL_GUIDE.md)
- **Training notebook**: See [test2.ipynb](test2.ipynb)

### Common Questions

**Q: Which model timestamp should I use?**
A: Use your latest model. Run `python test_hybrid_model.py --list` to see all available models.

**Q: Can I use different test data?**
A: Yes! Any static+temporal CSV pair works, or generate new with `--generate` flag.

**Q: How long does testing take?**
A: Usually 30-60 seconds for 500 students. More students = longer time.

**Q: What if hybrid model doesn't outperform?**
A: This suggests meta-learner isn't adding value. Review stacking architecture in training notebook.

**Q: Can I change the prediction threshold?**
A: Yes! The script shows threshold analysis. Edit threshold in prediction step (default: 0.5).

---

## üìÇ File Structure Summary

```
hybrid_framework/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README_TESTING.md              ‚Üê You are here
‚îú‚îÄ‚îÄ üìÑ QUICK_TEST_GUIDE.md            ‚Üê Quick reference
‚îú‚îÄ‚îÄ üìÑ TEST_MODEL_GUIDE.md            ‚Üê Detailed guide
‚îÇ
‚îú‚îÄ‚îÄ üêç test_hybrid_model.py           ‚Üê Main test script
‚îú‚îÄ‚îÄ üêç generate_synthetic_data.py     ‚Üê Data generator
‚îú‚îÄ‚îÄ üìì test2.ipynb                    ‚Üê Training notebook
‚îÇ
‚îú‚îÄ‚îÄ results/                          ‚Üê Trained models
‚îÇ   ‚îú‚îÄ‚îÄ lstm_model_20260111_023557.h5
‚îÇ   ‚îú‚îÄ‚îÄ rf_model_20260111_023557.pkl
‚îÇ   ‚îú‚îÄ‚îÄ gb_model_20260111_023557.pkl
‚îÇ   ‚îú‚îÄ‚îÄ meta_learner_20260111_023557.pkl
‚îÇ   ‚îî‚îÄ‚îÄ preprocessing_objects_20260111_023557.pkl
‚îÇ
‚îú‚îÄ‚îÄ uploads/                          ‚Üê Test data
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_static_*.csv
‚îÇ   ‚îî‚îÄ‚îÄ synthetic_temporal_*.csv
‚îÇ
‚îî‚îÄ‚îÄ test_results_*                    ‚Üê Generated reports & plots
    ‚îú‚îÄ‚îÄ test_results_roc_*.png
    ‚îú‚îÄ‚îÄ test_results_confusion_matrix_*.png
    ‚îú‚îÄ‚îÄ test_results_distributions_*.png
    ‚îî‚îÄ‚îÄ test_results_report_*.txt
```

---

## üéì Summary

You now have everything needed to test your hybrid model:

‚úÖ **3 comprehensive guides** (this file + 2 others)
‚úÖ **Automated test script** (test_hybrid_model.py)
‚úÖ **Data generator** (generate_synthetic_data.py)
‚úÖ **Clear success criteria** (75-85% accuracy expected)
‚úÖ **Visualization tools** (ROC curves, confusion matrices)
‚úÖ **Troubleshooting help** (common issues & solutions)

### Your Next Action
```bash
# Run this now to test your model:
python test_hybrid_model.py --timestamp 20260111_023557 --generate
```

**Good luck with your testing!** üöÄ

---

**Document Version**: 1.0
**Created**: January 11, 2026
**Last Updated**: January 11, 2026
**Tested With**: TensorFlow 2.x, Python 3.8+
